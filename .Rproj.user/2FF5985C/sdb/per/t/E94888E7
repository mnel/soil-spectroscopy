{
    "contents" : "\\documentclass[12pt, a4paper]{article}\n\\usepackage[sc]{mathpazo}\n\\renewcommand{\\sfdefault}{lmss}\n\\renewcommand{\\ttdefault}{lmtt}\n\\usepackage[T1]{fontenc}\n\\usepackage{geometry}\n\\usepackage{booktabs}\n\\usepackage{titlesec}\n\\heavyrulewidth = 2\\lightrulewidth\n\\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}\n\\setcounter{secnumdepth}{2}\n\\setcounter{tocdepth}{2}\n\\usepackage{url}\n\\usepackage[authoryear]{natbib}\n\\bibliographystyle{plainnat}\n\\usepackage[unicode=true,pdfusetitle,\n bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,\n breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]\n {hyperref}\n\\hypersetup{\n pdfstartview={XYZ null null 1}}\n\\usepackage{breakurl}\n\\usepackage[authoryear]{natbib}\n\\usepackage{titling}\n\\pretitle{\\begin{flushleft}\\LARGE}\n\\posttitle{\\par\\end{flushleft}\\vskip 0.5em}\n\\preauthor{\\begin{flushleft}\\large \\lineskip 0.5em}\n\\postauthor{\\par\\end{flushleft}}\n\\predate{\\begin{flushleft}\\itshape}\n\\postdate{\\par\\end{flushleft}\\rule{\\linewidth}{0.25mm}}\n%\\textwidth15.5cm\n\n\\newcommand{\\raca} {{\\sf RaCA}}\n\n\\newcommand{\\R} {{\\sf R}}\n\n\\setlength{\\parindent}{0pt}\n\\setlength{\\parskip}{2ex plus 0.5ex minus 0.2ex}\n\n\\titleformat{\\section}\n{\\normalfont\\Large\\sl}{\\thesection}{1em}{}\n\\titleformat{\\subsection}\n{\\normalfont\\large\\sf}{\\thesubsection}{1em}{}\n\\titleformat{\\subsubsection}\n{\\normalfont\\normalsize \\sc}{\\thesubsubsection}{1em}{}\n\\titleformat{\\paragraph}[runin]\n{\\normalfont\\normalsize \\itshape}{\\theparagraph}{1em}{}\n\\titleformat{\\subparagraph}[runin]\n{\\normalfont\\normalsize \\itshape}{\\thesubparagraph}{1em}{}\n\n\\titlespacing*{\\section} {0pt}{2ex plus 1ex minus .2ex}{2.3ex plus .2ex}\n\\titlespacing*{\\subsection} {0pt}{1.5ex plus 1ex minus .2ex}{1.5ex plus .2ex}\n\\titlespacing*{\\subsubsection}{0pt}{1.5ex plus 1ex minus .2ex}{1.5ex plus .2ex}\n\\titlespacing*{\\paragraph} {0pt}{1.5ex plus 1ex minus .2ex}{1em}\n\\titlespacing*{\\subparagraph} {\\parindent}{1.5ex plus 1ex minus .2ex}{1em}\n\n\\title{The \\raca{} project  ::  A soil spectral inference system using  legacy data}\n\\author{Michael Nelson, Budiman Minasny \\& Alex McBratney}\n\\date{Faculty of Agriculture, Food and Natural Resources \\\\ The University of Sydney}\n\n\\begin{document}\n\\baselineskip18pt\n% \\SweaveOpts{fig.path=figure/manual-,cache.path=cache/manual-,fig.align=center,external=TRUE,fig.show = hold}\n\n<<setup,echo=FALSE,results=hide,message=FALSE>>=\noptions(replace.assign=TRUE,width=90)\nknit_hooks$set(fig=function(before, options, envir){if (before) par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3)})\nsetwd('c:/Research/Spectra/calibrationReport')\nowd <- getwd()\n@\n\n\n\\maketitle\n\\section*{Summary}\n\\noindent\nThis document sets out the development of a soil spectral inference system as part of  the \\raca{} project. \nSpecifically it details the preparation of spectra and development of models for various soil properties using these processed visible--near-infrared (vis-NIR) spectra from a database of soil samples provided by the {\\sf USDA-NRCS} as part of the \\raca{} project.\nThese models will be used in the development of a classification models to pre-classify spectra collected through the  \\raca{} project to improve, i.e.\\ refine the soil carbon model.\n\n\n\\section{ Exploratory data analysis}\n\n<<setup-libraries, echo = F, cache=T,results=hide, warning =  F, message = F>>=\n## ########################################################\n## load the required libraries into R\nlibrary(signal) ; library(pls);  library(ggplot2); library(tikzDevice); library(treemap); library(gridExtra); library(munsell); library(Hmisc); library(Cubist); library(xtable); library(wavethresh); library(mvoutlier); library(stringr)\n##\n## ########################################################\n@\n\n<<functions-basic, echo = F, cache = T, warning = F, message = F, dependson = setuplibraries>>=\n## ########################################################\n##\n## some functions required for the calibration report for the RaCA project.\n##\n## ########################################################\nload_as <- function(.name, .file_path, .ext = '.RData'){\n  ## a function that loads a saved R object from a file\n  ## so you can rename it\n  load(file.path(as.character(.file_path), paste(as.character(.name),as.character(.ext), sep= '')))\n  .df <- get(as.character(.name))\n  return(.df)\n  }\n## ########################################################\nsave_as <- function(.object, .name, .file_path, .ext = '.RData',...){\n  ## a function that allows you to save an object to file, with a different name to \n  ## that within your workspace\n  ## object = an 'R' object\n  ## .fname = the file name you wish to save it as\n  ## .name = the name within that file\n  assign(.name, .object)\n  save(list = .name, file = file.path(as.character(.file_path), paste(as.character(.name), as.character(.ext), sep = ''))) \n  }\n## ########################################################\ntable_to_data.frame <- function(.x, .name = 'id'){\n  ## a function that creates a 1-d table\n  ## and convert it to a data frame\n  ## create table\n  table_x <- table(.x)\n  ## make data.frame\n  .df <- data.frame(.names = names(table_x), count = as.numeric(table_x))\n  names(.df)[1] <- .name\n  return(.df)\n}\n## ########################################################\nlength_NA <- function(.vector){length(na.omit(unlist(.vector)))}\n## ########################################################\nlength_unique <- function(.data,.variable,...){length(unique(.data[,.variable]))}\n## ########################################################\nbooktabs.xtable <- function(.xtable,...){\n  ## formats an xtable for book tabs\n  print(.xtable,sanitize.text.function = function(x){x},floating=FALSE,hline.after=NULL,\n                  add.to.row=list(pos=list(-1,0, nrow(.xtabl)),command=c('\\\\toprule ','\\\\midrule ','\\\\bottomrule ')))\n  }\n## ########################################################\ndrop0trailing.format <- function(x, ...) {\n  format(x,drop0trailing=T)\n  }\n## ########################################################\ncombine_list <- function(.x, .which_x, .y, .which_y, .id.var){\n  ## create list\n  list_x <- as.list(.x)\n  unique_id <- unique(list_x[[.id.var]])\n  ## add from\n  list_y <- as.list(.y[[unique_id]][.which_y])\n  c(list_x[.which_x],list_y)\n  }\n##\n## ########################################################\n\n@\n\n\n\n<<set-seed, echo = F,cache=T,results=hide, dependson = setup-libraries>>=\n## ########################################################\n##\n## Set the random seed. This allows any `random' components\n## to be repeated with the same results. \nset.seed(271011)\n##\n## set the memory limit as high as possible\nmemory.limit(size=4095)\n##\n## ########################################################\n@\n\n<<organize-create-directories, echo = F, cache = T, results = hide>>=\n## ########################################################\n##\n## create directories for saved data\nsaved_data_directory <- 'saved_data'\ndir.create(path = saved_data_directory)\n##\n## for raw spectra\nraw_spectra_directory <- file.path(saved_data_directory,'spectra_raw')\ndir.create( path = raw_spectra_directory)\n##\n## for processed spectra\nprocessed_spectra_directory <- file.path(saved_data_directory,'spectra_processed')\ndir.create(path = processed_spectra_directory)\n##\n## ########################################################\n@\n\n<<organize-file-locations, echo = F, cache = T, results = hide>>=\n## ########################################################\n##\n## set some locations on the computer\n##\n## base directory\nbase_directory <- 'c:/Research/Spectra'\n##\n## sub directory with reflectance spectra as ascii\nreflectance_directory <- file.path(base_directory, 'asciiSpectra')\n##\n## sub directory with absorbance spectra as ascii\nabsorbance_directory <-  file.path(base_directory,'asc1_R')\n##\n## file with sample information\nproperties_file <- 'new_soil_properties.csv' \n##\n## ########################################################\n@\n\n\n<<readin-soildata, echo = F, cache = T, results = hide, warning =  F, dependson = organize-file-locations>>=\n## ########################################################\n##\n## read in the sampled soil property data \n##\n## read in \nsoil_data <- read.csv(file.path(base_directory,properties_file), header=T)\n##\n## perform housekeeping\n##\n## remove 'dB columns\nsoil_data <- soil_data[,-c(17:21)]\n##\n## rename to a standard convention\nnew_names <- c('lab_project_name', 'user_pedon_id', 'lab_horizon_id', 'user_horizon_id',\n              'horizon_designation', 'horizon_top', 'horizon_bottom', 'vnir_name',\n              'total_C', 'CaCO3', 'est_org_C', 'clay', 'sand', 'cec_clay_ratio', \n              'taxonomy', 'lab_texture', 'pH_water', 'pH_CaCl', 'cec_NH4', 'fe_CD')\nnames(soil_data) <- new_names\n##\n## which columns are  the soil properties in?\nwhich_properties <- names(soil_data)[c(9:14,17:20)]\n## \n## save organized data.frame and clean up\nsave(soil_data,file= file.path(saved_data_directory,'soil_data.RData'))\ninvisible(gc())\n##\n## ########################################################\n@\n\n<<functions-summary, echo = F, cache = T, results = hide, dependson = functions-basic>>=\n## ########################################################\n##\n## functions to do with summarizing data\n## and organizing data for latex viewing\n##\n## ########################################################\nproperties_to_latex <- Vectorize(function(.name){\n  ## a function that goes from the column names to latex happy versions\nswitch(as.character(.name) ,\n  'clay' = 'Clay', 'sand' = 'Sand', 'total_C' = 'Total C', \n  'est_org_C' = 'Organic C', 'cec_clay_ratio' = 'CEC:Clay', \n  'pH_water' = 'pH (Water)', 'pH_CaCl' = 'pH $ \\\\left( {\\\\rm CaCl_2 } \\\\right)$',\n   'cec_NH4' = 'CEC', 'fe_CD' = 'Iron Oxide', 'CaCO3' = '${\\\\rm CaCO_3}$'      ) \n})\n## ########################################################\nn_unique_ids <- Vectorize(\n  function(.data, .var, .id){\n    length(unique(.data[!is.na(.data[,.var]), .id]))\n    }, \n  vectorize.args = '.var')\n## ########################################################\n@\n\n\n\n\n<<analysis-summarize-soil-samples, echo = F, results = hide, warning =  F,cache = F, fig.keep=none, dependson = functions-summary; readin-soildata>>=\n## ########################################################\n##\n## perform some summaries of the soil property data\n## \n## how many samples with spectra recorded\nn_spectra <- nrow(soil_data)\n##\n## melt soil_data for further analysis\nmolten_soil_variables <- melt(data.frame(soil_data,.row_index = 1:nrow(soil_data)), id.vars = c('user_pedon_id', 'lab_project_name','lab_horizon_id','.row_index'),measure.vars = which_properties, na.rm=T)\n##\n## how many samples recorded for each variable\nn_each_var <-  cast(molten_soil_variables, variable~.)\nnames(n_each_var)[2] <- 'count_samples'\n##\n## how many separate profiles  \nn_profiles <- length( levels( soil_data$user_pedon_id))\n##\n## and for each variable\nn_each_var$count_profiles <- daply(molten_soil_variables, .(variable), .fun = splat( function( user_pedon_id, ...){ count_profiles <- length(unique(user_pedon_id))}))\n##\n## how many profiles with taxonomy recorded\nn_profiles_taxonomy <- table_to_data.frame(ddply(soil_data, c('user_pedon_id'),   splat( function(taxonomy, ...){data.frame( taxonomy = unique(taxonomy))}))$taxonomy, .name  = 'taxonomy')[-1,]\n##\n## how many separate profiles with taxonomy recorded\ntotal_profiles_taxonomy <- sum(n_profiles_taxonomy$count)\n##\n## organize some of this information\nsample_numbers <- llist(n_spectra, n_profiles, n_each_var, n_profiles_taxonomy)\n##\n## save this information\nsave(n_profiles_taxonomy, file = file.path(saved_data_directory,'n_profiles_taxonomy.RData'))\nsave(sample_numbers, file  = file.path(saved_data_directory,'sample_numbers.RData'))\nsave(molten_soil_variables,file  = file.path(saved_data_directory,'molten_soil_variables.RData'))\n## \n## clean up\nrm(molten_soil_variables)\ninvisible(gc())\n##\n## ########################################################\n@\n\nIn its current form (\\today{}), the dataset contains \\Sexpr{n_spectra} samples and their spectra. \nThese samples come from \\Sexpr{n_profiles} profiles.\nSoil properties included at the moment are Total Carbon, ${\\rm CaCO_3}$, estimated Organic carbon, Clay, Sand, CEC : Clay ratio, Taxonomy, lab texture, pH in water, pH in ${\\rm CaCl_2}$, CEC and Fe. \nTotal Carbon and estimated organic carbon have values for all samples. Other soil properties have numerous missing values.\n\n\\subsection{USDA soil taxonomy}\nOf the \\Sexpr{n_profiles} individual profiles, \\Sexpr{total_profiles_taxonomy} have been classified to USDA soil taxonomy order level. \nFigure~\\ref{fig:taxonomy} shows the relative proportions of each USDA soil Taxonomy order for the \\Sexpr{total_profiles_taxonomy} profiles with recorded classifications. The recorded profiles are predominantly alfisols and mollisols.\n<<analysis-treeplot-taxonomy,echo=F,cache = T,results=hide,fig.keep=none>>=\n## ########################################################\n##\n## create a treemap of the taxonomy\n##\n## load the required data\nload('savedData/numbers.RData')\nload('savedData/summaryTax.RData')\n##\n## create the tree plot of the taxonomy  using tmPlot\ntree_taxonomy <- ddply(data.frame(tmPlot(n_profiles_taxonomy, 'taxonomy', \n  vSize = 'count', sortID = '-size',saveTm=T)[[1]][[1]], count = n_profiles_taxonomy$count),\n  .(ind), function(x){ data.frame(x, name_count =paste(x$ind,x$count,sep= '  \\n '))})\n##\n## this will be used with ggplot to make the map\n##\n## ########################################################\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-treeMapPlot, fig=T, dev=tikz,echo=F, out.width = \\textwidth, fig.width = 7, fig.height = 3.5,background = 1;1;1, cache = T,dependson = analysis-treeplot-taxonomy>>=\n## ########################################################\n##\n## use ggplot to plot the tree map for taxonomy\nggplot(tree_taxonomy, aes(xmin = x0,ymin = y0, ymax = y0 + h, xmax = x0 + w)) + \n  geom_rect(aes(fill = ind,col = ind )) + \n  geom_text(aes(x=x0+ w/2,y = y0 + h/2,label=name_count, size=count)) + \n  opts(axis.text.x = theme_blank(), axis.text.y = theme_blank(), \n    axis.ticks = theme_blank(), axis.title.y = theme_blank(), \n    axis.title.x = theme_blank(), panel.background = theme_blank(), \n    legend.position = 'none') +\n  scale_fill_manual(values = rev(brewer.pal(n=12,name='Set3')))+ \n  scale_colour_manual(values = rev(brewer.pal(n=12,name='Set3')))+\n  scale_size(to = c(2,7))+\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0))\n##\n## ########################################################\n@\n\\caption{Treemap showing the relative proportions of the \\Sexpr{total_profiles_taxonomy} profiles with recorded classifications}\\label{fig:taxonomy}\n\\end{figure}\n\n\\subsection{Soil attributes}\nTable~\\ref{table:nSamples} sets out the number of samples and unique samples for each laboratory-measured soil attribute. The distributions are shown in figure~\\ref{fig:histograms}\n\n\n\n\\begin{table}[tbp]\n\\caption{Number of samples and unique profiles for each soil attribute}\n\\label{table:nSamples}\n\\centering\n<<table-results-unique, echo=F, results = tex, cache = T , dependson = setup-libraries; functions-basic; analysis-summarize-soil>>=\n## ########################################################\n##\n## create a table of the profile counts\n## formatted for use with  booktabs in LaTeX\n##\n## get the  appropriate  rows\ntable_samples <- n_each_var[c(9:14,17:20),]      \n##\n## latexify names\nrow.names(table_samples) <- properties_to_latex(table_samples$.property)\nnames(table_samples)[2:3] <- c('Samples','Profiles')\n##\n## make xtable object\nxtable_samples <- xtable(table_samples[,2:3], digits=0)\n##\n## print nicely\nbooktabs.xtable(xtable_samples)\n##\n## ########################################################\n@\n\\end{table}\n\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-density-plots, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 7, fig.height = 9,background = 1;1;1, cache=T, dependson = analysis-create-histograms,warning=F>>=\n## ########################################################\n##\n## create and display density plots of the soil properties\n## using ggplot\n##\n## load required data\nload(file.path(saved_data_directory,'molten_soil_variables.RData'))\n##\n## add latex names\nmolten_soil_variables$latex_id <- with(molten_soil_variables,properties_to_latex(variable))\n##\n## make plot\nggplot(molten_soil_variables, aes(x=value)) + \n  geom_density() + \n  facet_wrap(~latex_id,scales='free', ncol=2)+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank())\n##\n## clean up\nrm(molten_soil_variables)\ninvisible(gc())\n## \n## ########################################################\n@\n\\caption{Density plots showing the distributions of the laboratory-measured soil attributes}\\label{fig:histograms}\n\\end{figure}\n\n\\section{Visible--near-infrared spectra}\nThe raw spectra were converted to ascii files of reflectance and absorbance using {\\sf ViewSpec Pro}. In \\R{}, we smooth the  vis-NIR spectra with a Sativksy-Golay filter, fitting quadratic polynomials to a moving 11-wavelength window. Using this smoothed data, we calculate the first derivative.\n\n<<functions-readin, echo=F, results = hide, cache = T, dependson = functions-basic;functions-summary>>=\n## ########################################################\n##\n## functions to read in spectral data from ascii files\n##\n## ########################################################\nread_column <- function(.id, .file_path, .ext, .column, header = T,...){\n  ##  a function to read in a single column from a csv file\n  read.csv(file.path(.file_path, paste(.id, .ext, sep = '')), header = header)[,.column]\n  }\n## ########################################################\nread_spectra <- function(.id, .file_path, .ext = '.txt', .column = 2, header = T, ...){\n  ## a function to read in spectra\n  ## from multiple files\n  ## read in using.id as the file name\n  spectra_list <- llply(.id, .fun = read_column, \n        .file_path = .file_path, .ext = .ext, .column = .column, header = header)\n  ## combine into a matrix\n  spectra <- do.call('rbind', spectra_list)\n  }\n## ########################################################\nread_save_spectra <- function(.id, .file_path, .ext = '.txt', .column = 2, header = T, .save_path, .type, .save_id, .save_ext = '.RData', ...){\n  ## a function to\n  ## read in spectral information, save it as an R object\n  ## and return the name and file path\n  ##\n  ## read in spectra\n  spectra <- read_spectra(.id = .id, .file_path = .file_path, .ext = .ext, .column = .column, header = T)\n  ## save spectra \n  ## create name\n  .save_name = paste(.type, .save_id, sep = '_')\n  ## save as\n  save_as(.object = spectra , .name = .save_name , .file_path = .save_path, .ext = .save_ext)\n  ## return the file names and paths\n  data.frame(.type, .spectra_path = .save_path, .spectra_name = .save_name, .spectra_ext =.save_ext)\n  }\n## ########################################################\ntrim_spectra <- function(.spectra_path, .spectra_name, .spectra_ext, .keep,.keep_id, .type, .save_path, .save_ext = '.RData', ... ){\n  ## ######################################################\n  ## a function to trim spectra to a particular          ##\n  ## subset of wavelengths (keep)                        ## \n  ## ######################################################\n  ## load spectra from file\n  spectra <- load_as(.name = .spectra_name, .file_path = .spectra_path, .ext = .spectra_ext)\n  ## trim\n  trimmed_spectra <- spectra[,.keep]\n  ## create object name\n  .save_name = paste(.type, .keep_id, sep = '_')\n  ## save \n  save_as(.object = trimmed_spectra , .name = .save_name , .file_path = .save_path, .ext = .save_ext)\n  ## return file names and paths\n  data.frame(.type, .keep_id, .spectra_path = .save_path, .spectra_name = .save_name, .spectra.ext = .save_ext)\n  }\n## ########################################################\nload_single_spectra <- function(.spectra_path, .spectra_name, .which,wavelengths,...){\n  ## a function to load a singe spectra from a file\n  spectra <- load_as(.file_path = .spectra_path, .name =.spectra_name)[.which,]\n  data.frame(value = spectra, wavelengths)\n}\n\n@\n\n<<organize-wavelength-info, echo = F, cache = T, results = hide>>=\n## ##################################################### ##\n## set out some useful variables and indicies            ##\n## to do with the wavelengths present in the spectra     ##\n## ##################################################### ##\n##\n## some useful values\nwavelengths <- 350:2500\n##\n## use 500 and 2451 as cut-offs  \nkeep_lengths <- (wavelengths >= 500) & (wavelengths <= 2451)\n##\n## a set that is dyadic\nkeep_dyadic <-  (wavelengths >= 404) & (wavelengths <= 2451)\n##\n## some indices for subsetting every tenth waveband\nevery_10 <- seq(1, 2151, by = 10)\nkeep_10  <- wavelengths %in% seq(400, 2450, by = 10)\n##\n##  place in a list for future use\nkeep_list <- list(keep = list(.keep = keep_lengths), \n                  dyadic = list(.keep = keep_dyadic), \n                  sub_10 = list(.keep = keep_10))\n##\n## ########################################################\n@\n\n\n\n<<readin-spectra, echo=F, results=hide, cache = T, dependson=functions-readin;readin-soildata; organize-wavelength-info>>=\n## ########################################################\n##                                                       ##   \n## read in the spectral information from ascii files     ##\n##                                                       ##\n## ########################################################\n##\n## read in all reflectance spectra spectra\n##\n## create the  argument list\nread_spectra_input <- list(reflect =list(.file_path = 'c:/Research/Spectra/asciiSpectra', .type = 'reflect'),             abs = list(.file_path = 'c:/Research/Spectra/asc1_R', .type = 'abs'))\n##\n## use this list to read in the spectra\nraw_spectra_files <- ldply(read_spectra_input, .fun = splat(read_save_spectra), .save_path = raw_spectra_directory,.ext = '.txt', .column = 2, header = T, .save_id = 'all', .id = soil_data$vnir_name)\n##\n## ########################################################\n@\n\n<<organize-spectra-trim, echo = F, results = hide, cache = T, dependson = functions-readin;readin-spectra;organize-wavelength-info>>=\n## ########################################################\n##                                                       ##   \n## Subset spectra using pre-defined sets of wavelengths  ##\n##                                                       ##\n## ########################################################\n##\n## create argument lists\ntrim_spectra_input  <-  alply(join(expand.grid(.type = c('reflect','abs'), \n  .keep_id = c('dyadic','keep','sub_10')),raw_spectra_files),1, combine_list,\n  .id.var = '.keep_id', .y = keep_list, .which_y = '.keep', \n  .which_x =  c(names(raw_spectra_files), '.keep_id'))\n##\n## using this argument list, trim the spectra\ntrimmed_spectra_files <- ldply(trim_spectra_input, .fun = splat(trim_spectra), .save_path = raw_spectra_directory,.ext = '.txt', .column = 2, header = T)\n##\n## clean up\ninvisible(gc())\n##\n## ######################################################## \n@\n\n<<functions-sg-filter, cache = T, results = hide, echo = F., dependson = functions-basic>>=\n## ########################################################\n##                                                       ##   \n## functions for sativsky-golay filtering                ##\n##                                                       ##\n## ########################################################\nfilter_sg <- function(.spectra_path, .spectra_name, .spectra_ext, .save_path, n, p, m, .type, ...){\n  ## a function to read a set ot spectra from a file\n  ## and apply the s-g filter with\n  ## parameters n, p, m (see help(sgolafilt))\n  ##\n  ## read in spectra\n  spectra <- load_as(.name = .spectra_name, .file_path = .spectra_path, .ext = .spectra_ext)\n  ##\n  ## run filter\n  sg   <- aaply(spectra, 1, sgolayfilt, n = n, p = p, m = m)\n  ##\n  ## arrange appropriately if a single sample\n  if(nrow(spectra) == 1){sg <- matrix(sg, dim(spectra))}\n  ##\n  ## name file\n  name_sg <- paste(.spectra_name,'_SGD', m, sep='')\n  ##\n  ## save\n  save_as(.object = sg,  .name = name_sg, .file_path = .save_path, .ext = .spectra_ext)\n  ##\n  ## return data frame of file name and locations\n  data.frame(.type, .filter = paste('SGD',m,sep=''), .spectra_name = name_sg, .spectra_path = .save_path, .spectra_ext)\n  }\n## ########################################################\n@\n\n<<organize-sg-filter, cache = T, echo =F, results = hide, dependson = functions-sg-filter; readin-spectra;organize-spectra-trim >>=\n## ########################################################\n##                                                       ##   \n## Filter the spectra using sativsky-golay filters       ##\n## with smoothing, and 1st derivative                    ##\n##                                                       ##\n## ########################################################\n##\n## organize arguments\nfilter_sg_input <-  join(raw_spectra_files, expand.grid(.type = c('reflect','abs'), m  = 0:1))\n##\n## fit s-g filter to relfectance and absorbance usng\n## the above arguments\nfilter_sg_files <- adply( filter_sg_input, 1, splat(filter_sg), n = 11, p = 2, .save_path = processed_spectra_directory)\n##\n## ########################################################\n##                                                       \n## s-g filter for the  every-10th wavelength subset spectra\n##\n## create argument list\nfilter_sg_sub10_input <- join(subset(trimmed_spectra_files,.keep_id=='sub_10'),  expand.grid(.keep_id='sub_10', m  = 0:1)) \n##\n## fit s-g filter to relfectance and absorbance usng\n## the above arguments\nfilter_sg_sub10_files <- adply(filter_sg_sub10_input, 1, splat(filter_sg), n = 3, p = 2, .save_path = processed_spectra_directory)\n##\n## ########################################################\n@\n\n\n\n\\subsection{Soil Colour}\nSoil colour was obtained by averaging the appropriate regions of each spectrum. Red $[600-690]$, green $[520-600]$ and blue $[450-520]$. These {\\sc rgb} values were converted to munsell colours using the {\\sf munsell} package in {\\sf R}. \nFigure \\ref{fig:munsellColour} shows the Munsell colours present within the dataset.\n\n<<functions-soil-colour, cache = T, results = hide, warning = F, fig.keep = none, dependson = functions-basic>>=\n## ########################################################\n##                                                       ##   \n## functions for obtaining colour  from spectra          ##\n##                                                       ##\n## ########################################################\n## ########################################################\nin_interval <- function(.all,.interval,...){\n  ## index for an interval\n  ## a function to subset a particular waveband interval\n .in_interval = .all %in% .interval\n}\n## ########################################################\nmean_interval <- function(.data, .index){\n  ## returns the mean for  given indices\n  mean(.data[.index])\n}\n## ########################################################\nspectra_to_RGB <- function(spectra, all_wavelengths, \n   rgb_list = list(blue = list(.interval = 450:520), red = list(.interval =600:690), green = list(.interval =520:600)), ...) {\n  ## a function to return the average values in the \n  ## red, green and blue sections of the spectrum\n  ## would work on any intervals\n  ##\n  ## get the appropriate indices\n  interval_list <-llply(rgb_list,splat(in_interval), .all =all_wavelengths)\n  ##\n  ## get the average in these subsets\n  rgb_values <- lapply(interval_list, mean_interval, .data =spectra)\n  ##\n  ## convert to colour\n  colour <- with(rgb_values,rgb(red, green, blue))\n  ##\n  ## return data frame\n  with(rgb_values,data.frame(red, green, blue, colour))\n  }\n## ########################################################\nsoil_colour_spectra <- function(.spectra_name, .spectra_path, .spectra_ext, wavelengths,...){\n  ## this function loads spectra from a file\n  ## and returns the rgb colour and munsell colour\n  ##\n  ## read spectra\n  spectra <-  load_as(.name = .spectra_name, .file_path = .spectra_path, .ext = .spectra_ext)\n  ##\n  ## find r,g,b colour\n  rgb_colours <- adply(spectra, 1, spectra_to_RGB, all_wavelengths = wavelengths)\n  ##\n  ## get munsell colour\n  munsell_colours <- splat(function(red,green,blue,...){rgb2mnsl(R=red,G=green,B=blue)})(rgb_colours)\n  ##\n  ## return \n  data.frame(rgb_colours, munsell = munsell_colours)\n  }\n## ########################################################\n@\n\n\n<<analysis-soilColour,echo=F, cache = T, results=hide, background = 1;1;1,warning=F, fig.keep=none; dependson = functions-soil-colour; readin-spectra>>=\n## ########################################################\n##                                                       ##\n## analyze the soil colour for the reflectance spectra   ##\n##                                                       ##\n## ########################################################\n##\n## calculate the RGB + munsell colours \nsoil_colour <- splat(soil_colour_spectra)(raw_spectra_files[1,],wavelengths = wavelengths)\n##\n## save\nsave_as(soil_colour, .file_path = saved_data_directory, .name= 'soil_colour')\n##\n## clean up\nrm(soil_colour)\ninvisible(gc())\n##\n## ########################################################\n@\n\n<<analysis-treemap-munsell,echo = F, cache = T, results=hide, dependson = analysis-soilColour>>=\n## ########################################################\n##                                                       ##\n## create a tree map of the munsell colours              ##\n##                                                       ##\n## ########################################################\n##\n## load the data \nsoil_colour <- load_as(.file_path = saved_data_directory, .name= 'soil_colour')\n##\n## summarize by colour\nmunsell_summary <- table_to_data.frame(soil_colour$munsell,.name='munsell')\n##  \n## create a tree map plot\ntree_munsell <- data.frame(tmPlot(munsell_summary,'munsell', vSize = 'count',sortID = '-size',saveTm=T)[[1]][[1]],                          count = munsellDF$count)\n##\n## convert to hex colour for plotting\ntree_munsell$hex_colour <- mnsl2hex(tree_munsell$ind)\n## and create labels for plotting\ntree_munsell$name_count <- ddply(tree_munsell,1,function(x){paste(x$ind,x$count,sep= '  \\n ')})$V1\n##\n## clean  up\nrm(list = c('munsell_summary','soil_colour'))\ninvisible(gc())\n## ########################################################\n@\n\\begin{figure}[tbp]\n\\centering\n<<figure-treemap-munsell,  fig=T, dev=tikz,echo=F, out.width = 0.8\\textwidth, fig.width = 7,fig.height=4.3268,cache=T, background = 1;1;1; dependson = analysis-treemap-munsell>>=\n## ########################################################\n## plot the tree map using ggplot2                       ##\n## ########################################################\n##\nggplot(tree_munsell, aes(xmin = x0,ymin = y0, ymax = y0 + h, xmax = x0 + w)) + \n  geom_rect(aes(fill = hex_colour,colour=hex_colour )) + \n  scale_fill_identity()+\n  scale_colour_identity()+\n  geom_text(aes(x=x0+ w/2,y=y0+h/2,label = name_count, size=count))+\n  opts(axis.text.x = theme_blank(), axis.text.y = theme_blank(), axis.ticks = theme_blank(), \n    axis.title.y = theme_blank(), axis.title.x = theme_blank(), panel.background = theme_blank(), \n   legend.position = 'none') +\n  scale_size(to = c(1,7))+\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0))\n## ########################################################\n@\n\\caption{Treemap showing the relative proportions of the munsell colours derived from spectroscopic measurements of soil colour for each sample profiles }\\label{fig:munsellColour}\n\\end{figure}\n\n\n\\section{Initial predictive models for soil attributes using vis-NIR spectra}\n\\subsubsection{Partial least squares regression}\nThe first approach attempted for the predictive models was Partial least squares (PLS) regression on the first derivative of the S-G filtered data which had been sampled at every 10th wavelength.\nThe results for the various PLS models are showin in figures~\\ref{fig:rmsePls} (RMSE) \\&~\\ref{fig:r2Pls} ($R^2$).\n\n<<functions-sampling-rate, cache = T, echo = F, results = hide, warning = F, dependson = functions-basic>>=\n## ########################################################\n## functions for sampling                                ##\n## basic sampling at a prescribed rate                   ##\n## ########################################################\n\ntraining_info_rate <- function(value, variable, .id_column, .rate, .save_path, ...){\n  ## a function to create basic sampling by rate\n  ##\n  ## get the column which includes the row identifiers\n  other_args <- list(...)\n  id_samples <- other_args[[.id_column]]\n  ##\n  ## how many samples in the training data?\n  n_samples <- floor(.rate * length(value))\n  ##\n  ## which samples are in the training data\n  sample_rows <- sample.int(length(value), size = n_samples)\n  ## \n  ## which ids are these?\n  id_training <- id_samples[sample_rows]\n  ##\n  ## which ids in the validation data\n  id_validation <- id_samples[-sample_rows]\n  ##\n  ## training and sampling row ids\n  rows_training <- sample_rows\n  rows_validation <- (1:length(value))[-sample_rows]\n  ##\n  ## get the data values in the training, validation\n  ## and all data (no NAs!)\n  data_training <- value[sample_rows]\n  data_validation <- value[-sample_rows]\n  data_all <- value\n  ## \n  ## place in a named list\n  training_info <- llist(id_training, id_validation, data_training, data_validation, data_all,id_samples,rows_training, rows_validation)\n  ##\n  ## name and save\n  .property <- unique(as.character(variable))\n  obj_name <- paste(.property,'rate','training_info',sep='_')\n  save_as(.object = training_info, .file_path = .save_path, .name= obj_name)\n  ##\n  ## make sure the property id is there\n  training_info$variable <- .property\n  ##\n  ## return\n  return(training_info)\n }\n## ########################################################\n@\n\n<<functions-models-general, cache = T, echo = F, results = hide, warning = F, dependson = functions-basic>>=\n## ########################################################\n## functions for modelling in general                    ##\n## including summarizing with MAE and RMSE               ##\n## ########################################################\nmodel_summary <- function(.which,observed,predicted){\n     ## returns MAE and RMSE for a subset of data\n     modelResid <- (observed-predicted)[.which]\n     data.frame(MAE = mean(abs(modelResid)), RMSE = sqrt(mean(modelResid^2)))\n  }\n## ########################################################\n@\n\n\n\n<<functions-models-plsr, cache = T, echo = F, results = hide, warning = F, dependson = functions-basic, functions-models-general>>=\n## ########################################################\n## functions for modelling  with plsr                    ##\n## including summarizing with MAE and RMSE               ##\n## ########################################################\nfit_pls <- function(id_training, id_validation, data_training, data_validation, data_all,id_samples,rows_training, rows_validation, variable, \n  .spectra_name, .spectra_path, .save_path,.type, .filter,.keep, .sampling, ...){\n  ##  \n  ## load spectra\n  spectra <- load_as(.file_path = .spectra_path, .name = .spectra_name)[id_samples, .keep]\n  ##\n  ## organize data\n  pls_data <- data.frame(.var = data_all, spectra = I(as.matrix(spectra)))\n  ##\n  ## rename\n  names(pls_data)[1] <- as.character(variable)\n  ##\n  ## training data\n  pls_training <- data.frame(.var = data_training, spectra = I(as.matrix( spectra[rows_training,])))\n  ##\n  ## rename\n  names(pls_training)[1] <- as.character(variable)\n  ##\n  ## fit model\n  pls_model <- plsr(as.formula(paste(as.character(variable), '~', 'spectra', sep = ' ')), data = pls_training)\n  ##\n  ##  save model\n  model_name <- paste(as.character(variable),'PLS',.type,.filter,.sampling,sep ='_')\n  save_as(.object =pls_model, .file_path = .save_path, .name = model_name)\n  ##\n  ## predict at all data\n  all_predict <- predict(pls_model, newdata = pls_data)[,1,]\n  ##\n  ## summarize results\n  summary_results <- adply(all_predict,2, function(x){\n  ldply(list(training = rows_training, validation = rows_validation), \n    model_summary, observed = data_all, predicted = x)\n  })\n  names(summary_results)[2] <- 'dataset'\n  ##\n  ## save summary\n  summary_name <- paste(as.character(variable),'PLS_summary',.type,.filter,.sampling,sep ='_')\n  save_as(.object =summary_results, .file_path = .save_path, .name = summary_name)\n  ##\n  ## return summary_results\n  data.frame(summary_results, .type, .sampling, .filter, variable = as.character(variable))\n  ##\n  }\n## ########################################################\n@\n\n<<organize-models-create-directories, echo = F, cache = T, results = hide,dependson = organize-create-directories>>=\n## ########################################################\n## creae the directories to save the models and results  ##\n## ########################################################\n## create directories for training information\ntraining_info_directory <- file.path(saved_data_directory,'training_info')\ndir.create(training_info_directory)\n##\n## for the models\nmodels_directory <- file.path(saved_data_directory,'models')\ndir.create(models_directory)\n##\n## for pls\npls_models_directory <- file.path(models_directory,'plsr')\ndir.create(pls_models_directory)\n##\n## for cubist\ncubist_models_directory <-  file.path(models_directory,'cubist')\ndir.create(cubist_models_directory)\n##\n## ########################################################\n@\n\n<<organize-sampling-rate, echo = F ,results = hide, cache = T, dependson = organize-models-create-directories; functions-sampling-rate>>=\n## ########################################################\n## create training and validation datasets using         ##\n## basic sampling at a prescribed rate                   ##\n## ########################################################\n##\n## load the required molten data \n load(file.path(saved_data_directory,'molten_soil_variables.RData'))\n##\n## create the training information and dataset\nrate_training_info_list <- dlply(molten_soil_variables, .(variable), \n  splat(training_info_rate), .save_path =training_info_directory, \n  .id_column = '.row_index',.rate = 0.7) \n##\n## save this training information list for future use\nsave(rate_training_info_list, file = file.path(training_info_directory,paste('rate_training_info_list.RData')))\n##\n## clean-up\nrm(list = c('rate_training_info_list','molten_soil_variables.RData' ))\ninvisible(gc())\n## ########################################################\n@\n\n\n\n\n<<analysis-PLSR-basic-fit, echo=F, results=hide, warning =F, cache = T; dependson = readin-soildata; functions-basic, functions-plsr; readin-spectra>>=\n## ########################################################\n## fit the pls models to the rate-sampled datasets       ##\n## for each soil property  using SGD1 filtered spectra   ##\n## sampled at every 10th wave length                     ##\n## including summarizing with MAE and RMSE               ##\n## ########################################################\n##\n## load training info\ntraining_info_list <- load_as(.file_path = training_info_directory, .name = 'rate_training_info_list')\n##\n## create the  argument list using SGD1 filtered soils\npls_model_arguments <- c(llply(training_info_list, function(x,y){c(x,as.list(y))},\n  y = filter_sg_sub10_files[2,]), llply(training_info_list, function(x,y){c(x,as.list(y))},\n  y = filter_sg_sub10_files[4,]))\n##\n## fit model to reflectance and fit models to absorbance and reflectance\npls_model_rate_summaries  <-  ldply(pls_model_arguments , splat(fit_pls),\n                  .keep = 1:206,  .save_path = pls_models_directory, .sampling = 'rate')\n##\n## save argument list\nsave(pls_model_arguments, file = file.path(pls_models_directory, paste('pls_model_arguments.RData')))\n##\n## and model summaries\nsave(pls_model_rate_summaries, file = file.path(pls_models_directory, paste('pls_model_rate_summaries.RData')))\n##\n## clean up\nrm(list = c('pls_model_rate_summaries','training_info_list','pls_model_arguments'))\ninvisible(gc())\n## ########################################################\n@\n\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-PLSR-rmse,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 7,fig.height=9,cache =T, background = 1;1;1, warning = F, dependson = setup-libraries;analysis-PLSR-basic-summary>>=\n## ########################################################\n## create a plot of the RMSE for the different           ##\n## models for each soil property                         ##\n## ########################################################\n##\n## load  required data \nload(file.path(pls_models_directory,paste('pls_model_rate_summaries.RData')))\n##\n## a bit of fixing to allow nice plotting with tikz / latex\n## \n## the soil property names\npls_model_rate_summaries$latex_id <- with(pls_model_rate_summaries, properties_to_latex(variable))\n##\n## make the number of pls components numeric\npls_model_rate_summaries$components <- laply((str_split(pls_model_rate_summaries$X1,pattern = ' ')), function(.x){as.numeric(.x[1])})\n##\n## create the plot\ntheme_set(theme_bw())\nggplot(pls_model_rate_summaries, aes(x = components, y = RMSE)) + \n    facet_wrap(~latex_id,ncol=2,scales ='free_y') +\n    geom_line(aes(colour= .type:factor(dataset))) +\n    opts(strip.background = theme_blank()) +\n    scale_colour_manual(values = c(\"red\",\"blue\",'green','orange')) +\n    ylab('RMSE')\n##\n## clean up\nrm(list = c('pls_model_rate_summaries'))\ninvisible(gc())\n## ########################################################\n@\n\\caption{Root-mean-square error from PLSR models for training and validation data sets for the various soil properties}\\label{fig:rmsePls}\n\\end{figure}\n\n\n\n\n\\subsubsection{Cubist}\nA second approach was to use  the Cubist algorithm \\citep{Kuhn2011}, which builds parsimonious regression trees using the variables with linear regression models at each node. \nHere, we implemented the algorithm with 5 committees. The RMSE for the training and test datasets are in table~\\ref{table:cubistRMSE}. As with the PLS models, these were computed using every tenth wavelength and the first derivative of the S-G filtered spectra.\n\n<<functions-models-cubist-basic, cache = T, echo = F, results = hide, depends = functions-basic; functions-models-general>>=\n## ########################################################\n## functions for fitting cubist models                   ##\n## and summarizing!                                      ##\n## ########################################################\nfit_cubist <- function(id_training, id_validation, data_training, data_validation, \n  data_all, id_samples, rows_training, rows_validation, variable, .spectra_name, .spectra_path,\n  .save_path,.type, .filter,.keep, .sampling, .coef_names,.cubist_control = cubistControl(), .committees, ...){\n  ## a function that fits a cubist model for a variable and spectra data  \n  ##\n  ## load spectra\n  spectra <- load_as(.file_path = .spectra_path, .name = .spectra_name)[id_samples, .keep]\n  ##\n  ## get training data -- y\n  cubist_y <- data_training\n  ##\n  ## get training data  -- x\n  cubist_x <- data.frame(spectra[rows_training,])\n  ##\n  ## give names that make sense\n  names(cubist_x) <- .coef_names\n  ##\n  ## fit model\n  cubist_model <- cubist(y = cubist_y, x = cubist_x, control = .cubist_control, committes = .committees)\n  ##\n  ## name and save the  model\n  model_name <- paste(as.character(variable),'cubist',.type,.filter,.sampling, sep ='_')\n  save_as(.object =cubist_model, .file_path = .save_path, .name = model_name)\n  ##\n  ## ######################################################\n  ##\n  ## model summaries\n  ##\n  ## create newdata object for predictions\n  cubist_x_all <- data.frame(spectra)\n  names(cubist_x_all) <- .coef_names\n  ##\n  ## predict at all data locations\n  predict_all <- predict(cubist_model, newdata = cubist_x_all)\n  ##\n  ## get model statistics\n  summary_results <- ldply(list(training = rows_training, validation = rows_validation), .fun = model_summary, observed = data_all, predicted = predict_all)\n  names(summary_results)[1] <- 'dataset'\n  ##\n  ## return summary_results\n  data.frame(summary_results, .type, .sampling, .filter, variable = as.character(variable))\n  ##\n  }\n## ########################################################\n@\n\n\n<<analysis-cubist-basic-fit, echo=F, results=hide, warning =F, cache = T; dependson = functions-models-cubist-basic; organize-sampling-rate>>=  \n## ########################################################\n## fit models using cubist                               ##\n## basic rate sampling                                   ##\n## to all soil properties                                ##\n## using SG filtered-D1 reflectance and absorbance       ##\n## ########################################################\n##\n## load training info\ntraining_info_list <- load_as(.file_path = training_info_directory, .name = 'rate_training_info_list')\n##\n## create names for coefficients\ncoef_names <-paste('w', seq(400, 2450, by = 10), sep = '') \n##\n## create argument lists for fitting cubist models to \n## SG filtered-D1 reflectance and absorbance\ncubist_basic_arguments <- c(llply(training_info_list, function(x,y){c(x,as.list(y))},\n  y = filter_sg_sub10_files[2,]), llply(training_info_list, function(x,y){c(x,as.list(y))},\n  y = filter_sg_sub10_files[4,]))\n##\n# fit the models and get summaries \nbasic_cubist_summaries <-  ldply(cubist_basic_arguments,.fun = splat(fit_cubist), .coef_names = coef_names, .keep = 1:206, .save_path = cubist_models_directory, .committees = 5, .sampling = 'rate')\n##\n## save\nsave(cubist_basic_arguments, file = file.path(cubist_models_directory, 'cubist_basic_arguments.RData' ))\nsave(basic_cubist_summaries, file = file.path(cubist_models_directory, 'basic_cubist_summaries.RData' ))\n##\n## clean-up\nrm(list = c('cubist_basic_arguments','basic_cubist_summaries'))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\n\\begin{table}[tbp]\n\\caption{RMSE for validation and training data sets using cubist.}\n\\label{table:cubistRMSE}\n\\centering\n<<table-cubist-basic-summary, echo = F, results =tex, cache = T, dependson = analysis-cubist-basic-summary>>=\n## ########################################################\n## create a table summarizing the cubist fits            ##\n## ########################################################\n##\n## load data\ncubist_basic_summary <- load_as(.file_path = cubist_models_directory, .name = 'basic_cubist_summaries')\n##\n## reformulate results table\nbasic_cubist_table <- cast(reshape2:::melt.data.frame(cubist_basic_summary, measure.var = 'RMSE', variable.name = 'statistic', id.var = c('.type', 'dataset', 'variable')), variable  ~.type +dataset)\n##\n## set row names for xtable / latex\nrow.names(basic_cubist_table) <- properties_to_latex(basic_cubist_table$variable)\n##\n## create nicer looking column names\nnames(basic_cubist_table)[-1] <-laply(str_split(names(basic_cubist_table)[2:5], pattern = '_'), function(x){paste(x[1], ' (', x[2],')',sep='')})\n##\n## latexify table using xtable\nbasic_cubist_xtable <- xtable(basic_cubist_table[,-1], digits=3)\n##\n## and print this  using booktabs format\nbooktabs.xtable(basic_cubist_xtable)\n##\n## clean up\nrm(list = c('basic_cubist_summary', 'basic_cubist_table','basic_cubist_xtable'))\ninvisible(gc())\n## ########################################################\n@\n\\end{table}\n\n\\subsubsection{Overview of initial models}\nFrom figure~\\ref{fig:rmsePls}  and table~\\ref{table:cubistRMSE}, the Cubist models appear to be generally more robust than the PLS regression, especially to prediction on the validation data set. Models using the absorbtion tend to perform better than the reflectance on the training data sets, but not on the validation data sets.\n\nIn later sections we consider using continuum-removed spectra and wavelet decomposition in developing the predictive models. We will also consider various sampling techniques and outlier detection and removal.\n\n\n\\section{Clay mineralogy}\nFollowing the work of \\citet{ViscarraRossel2009} and \\citet{ViscarraRossel2011}, we consider calculating the relative abunance of the clay minerals smectite, illite and kaolinite. \nThis procedure involves calculating the continuum-removed spectra for various ranges of wavelengths $[2130 - 2260], [2260-2410]$ and $[1830 - 2130]$. \nIn this case, contiuum removal process was to calculate deviations from the convex hull \\citep{Clark1984}.\n\n\n<<functions-mineralogy, cache = T, echo = F, results = hide, dependson = functions-soil-colour>>=\n## ########################################################\n## some functions for analysing mineralogy from spectra  ##\n## ########################################################\nadd_interval_info <- function( .id, .from, ...){\n  ## defunct function for grabbing interval information\n  ## convert the data frame row to a list\n  .list <- as.list(...)\n  ## get the appropropriate info\n  .list$.interval = .from[[.id]]$.interval\n  .list$.id. <- .id\n  .list\n  }\n## ########################################################\nrename_in <- function(.list, .old, .new,...){\n  ## defunct function for renaming in a list\n  names_list <- names(.list)\n  which_change <- which(names_list %in% .old)\n  names_list[which_change] <- .new\n  names(.list) <- names_list\n  .list\n  }\n## ########################################################  \nread_USGS <- function(.spectra_fname){\n   ## a function to read in a spectra downloaded from USGS\n  .data <- read.table(file = .spectra_fname, skip=17, na.strings = '-1.23e34', \n                      col.names = c('wavelength', 'reflectance','sd'))\n  .data[,1] <- .data[,1] * 1000\n  .data\n  }\n## ########################################################\nread_reference_spectra <- function(.spectra_fname, .all, mineral, .save_path, .type,  ...){\n  ## a function to read in and smooth reference spectra \n  ## from USGS\n  ##\n  ## read in spectra\n  usgs_spectra <- readUSGS(as.character(.spectra_fname))\n  ##\n  ## smooth to all wavebands (usually 1-nm 350:2500)\n  spline_smooth <- spline(usgs_spectra[,1:2], xout = .all)\n  ##\n  ## make a matrix with 1 row\n  spectra <- matrix(spline_smooth$y,nrow = 1)\n  ##\n  ## name and save\n  reference_name <- paste(mineral,'reference',.type,sep='_')\n  save_as(.object = spectra, .file_path = .save_path, .name = reference_name)\n  ##\n  ## return file name and location\n  data.frame(.spectra_path = .save_path, .spectra_name = reference_name, mineral, .type)\n  ##\n  }\n## ########################################################\n@\n\n<<organize-mineralogy-clay, echo = F, cache = T, results = hide, dependson = organize-wavelength-info; functions-mineralogy>>=\n## ########################################################\n## set the mineralogical infomation used in              ##\n## analysing the spectra for different clay minerals     ##\n## ########################################################\n##\n## organize the data for mineralogical analysis\nclay_minerals_list <- list(smectite = list(.interval = 1830:2130, mineral = 'smectite', .diagnostic = 1912),                            kaolinite = list(.interval = 2131:2260, mineral = 'kaolinite', .diagnostic = 2165), illite = list( .interval = 2261:2410, mineral = 'illite', .diagnostic = 2345)) \n## ########################################################\n@\n\n<<readin-clay-reference-spectra, cache = T, echo = F, results = hide, dependson = continuum-removal; functions-mineralogy>>=\n## ########################################################\n## read in the reference spectra for clay minerals       ##\n## from USGS spectra                                     ##\n## process using s-g filter                              ##\n## ########################################################\n## \n## set the filenames of reference spectra\nclay_reference_list <- list(smectite = list( .spectra_fname = 'montmorillonite_sca2.14557.asc', mineral = 'smectite',       .type = 'reflect'), kaolinite = list(.spectra_fname = 'kaolinite_cm9.11962.asc', mineral = 'kaolinite', .type= 'reflect'),   illite = list(.spectra_fname = 'illite_imt1.10982.asc', mineral = 'illite', .type = 'reflect'))\n##\n## read in spectra and save\nclay_reference <- ldply(clay_reference_list, splat(read_reference_spectra), .all = wavelengths,                       .save_path = raw_spectra_directory)\n##\n## process with sg-filter\nclay_reference_sg <- ddply(clay_reference, .(mineral), splat(filter_sg), n = 11, p = 2, m = 0,                           .save_path =  processed_spectra_directory, .spectra_ext = '.RData')\n##\n## ########################################################\n@\n\n<<functions-filter-continuum-removal, cache = T, echo = F, results = hide, dependson = functions-basic; functions-mineralogy>>=\n## ########################################################\n## some functions for filtering using                    ##\n## deviations from the convex hull                       ##\n## also known as continuum removal                       ##\n## ########################################################\nc_hull_deviation <- function(.spectra, .all = 350:2500, .interval = 350:2500, .return_hull = F,...){\n  ## a function to perform deviations from the convex hull\n  ## at the moment only tested on reflectance values\n  ## not absorbance\n  ##\n  ## organize data\n  ##\n  ## get the interval\n  .in_interval <- in_interval(.all = .all, .interval = .interval)\n  ##\n  ## sort the data\n  .data <- sortedXyData(.all[.in_interval], .spectra[.in_interval])\n  ##\n  ## calculate convex rull\n  c_hull <- chull(.spectra[.in_interval])\n  ##\n  ## get the approprite region\n  c_hull <- c_hull[which(c_hull == 1):length(c_hull)]\n  ##\n  ## calculate linear approximation between hull points\n  linear_approx <- approx(.data[c_hull,], xout = .interval, method = 'linear', ties = 'mean')\n  ##\n  ## calculate the deviation from the convex hull\n  deviation <-  ( linear_approx[[2]] - .spectra[.in_interval] )/linear_approx[[2]]\n  ##\n  ## add the hull if you wish\n  if(.return_hull == T){attr(deviation, 'hull') <-linearApprox[[2]]}\n  ##\n  ## return\n  return(deviation)\n  ##\n  }\n## ########################################################\nfilter_continuum_removal <- function(.spectra_path, .spectra_name, .interval, mineral = 'all', \n      .all, .return_hull = F,.save_path,.type, .filter, ...){\n  ## a function to perform continuum removal on reflectance\n  ## first loading in the spectra\n  ##\n  ## load spectra\n  spectra <- load_as(.file_path = .spectra_path, .name = .spectra_name)\n  ##\n  ## calculate deviations from the convex hull for each .row\n  ## use aaply to avoid having to transpose the outcome\n  spectra_cr <- aaply(spectra, 1, c_hull_deviation, .interval = .interval, .all = .all, .return_hull = F)\n  ##\n  ## rename and save\n  .new_filter <- paste(.filter, 'cr', sep='_')\n  processed_spectra_name <- paste(.spectra_name,'cr',mineral, sep='_')\n  save_as(.object = spectra_cr, .name = processed_spectra_name, .file_path = .save_path)\n  ##\n  ## return  data frame of file locations and object names\n  data.frame(.filter = .new_filter, .spectra_path = .save_path, .spectra_name = processed_spectra_name,  .type, mineral )\n  ##\n  }\n## ########################################################\nspectral_quantiles <- function(.spectra_path, .spectra_name, .filter, ., probs, .interval, .save_path, .type,mineral = 'all', ...){\n  ## a function to get quantiles of the spectra for\n  ## a given interval (mineral)\n  ##\n  ## load spectra\n  spectra <- load_as(.file_path = .spectra_path, .name = .spectra_name)\n  ##\n  ## calculate some quantiles\n  .quantiles <- adply( spectra, 2, quantile, prob = probs)\n  ##\n  ## rename the variable \n  names(.quantiles)[1] <- 'wavelength';\n  ##\n  ## and give the correct wavelength IDs\n  .quantiles$wavelength = .interval\n  ##\n  ## melt\n  molten_quantiles <- melt(.quantiles, id.var = 'wavelength')\n  ##\n  ## reassign levels\n  levels(molten_quantiles$variable) <- probs\n  ##\n  ## rename and save\n  quantile_name <- paste(.type,.filter,mineral,'quantiles',sep='_')  \n  save_as(.object = molten_quantiles, .file_path = .save_path, .name = quantile_name)\n  ##\n  ## return the molten quantiles\n  data.frame(molten_quantiles)\n  ##\n  }\n## ########################################################\n@\n\n<<analysis-mineralogy-clay-continuum-removal, echo=F, cache = T, results= hide; dependson = organize-mineralogy-clay; functions-filter-continuum-removal>>=\n## ########################################################\n## continuum removal on  particular waveband intervals   ##\n## corresponding to different clay minerals              ##\n## ########################################################\n##\n## organize the filter arguments as a list   \nfilter_cr_clay_arguments <- llply(clay_minerals_list, function(x,y){c(x,as.list(y))}, y = filter_sg_files[1,])   \n##\n## run the continuum removal\nclay_minerals_cr_files <-ldply(filter_cr_clay_arguments, splat(filter_continuum_removal), .save_path = processed_spectra_directory, .all = wavelengths)\n##\n## ########################################################\n## Continuum removal for reference spectra for the\n## clay minerals\n## #######################################################\n## \n## organize the arguments as a list\nclay_reference_cr_list <-  alply( clay_reference_sg, 1, combine_list, .which_x = names(clay_reference_sg[,-1]), .y = clay_minerals_list, .which_y = names(clay_minerals_list[[1]]), .id.var = 'mineral')\n## \n## run the continuum-removal \nclay_reference_cr_files <-ldply(clay_reference_cr_list, splat(filter_continuum_removal), .save_path = processed_spectra_directory, .all = wavelengths)\n##\n## #######################################################\n@\n\n<<organize-spectra-quantiles-clay, echo = F, cache = T, dependson = analysis-mineralogy-clay-continuum-removal>>=\n## ########################################################\n## create quantiles of the continuum-removed spectra     ##\n## for plotting against the reference spectra            ##\n## ########################################################\n##\n## organize the arguments\nclay_quantiles_cr_arguments <-  alply( clay_minerals_cr_files, 1, combine_list, .which_x = names(clay_reference_sg[,-1]), .y = clay_minerals_list, .which_y = names(clay_minerals_list[[1]]), .id.var = 'mineral')\n##\n## get the quantiles of the cr-spectra for the samples\nclay_cr_quantiles <- ldply(clay_quantiles_cr_arguments, splat(spectral_quantiles), probs =c(0.05,0.06, 0.5,0.84, 0.95),  .save_path = processed_spectra_directory)\n##\n## ########################################################\n## get the reference spectra in the same format   \n##\n## an argument list\nclay_reference_arguments <-  alply( clay_reference_cr_files, 1, combine_list, .which_x = names(clay_reference_sg[,-1]), .y = clay_minerals_list, .which_y = names(clay_minerals_list[[1]]), .id.var = 'mineral')\n##\n## and reading and organizing\nclay_reference_spectra <- ldply( clay_reference_arguments, splat (function(.spectra_path, .spectra_name, .interval, mineral, ...){ data.frame(value = load_as(.file_path = .spectra_path, .name = .spectra_name), wavelength = .interval,.mineral = mineral)}))\n##\n## ########################################################\n## and the diagnostic bands\n##\n## set up diagnostic bands as a data frame\ndiagnostic_clay <- ldply(clay_minerals_list,splat(function(mineral,.diagnostic,...){data.frame(mineral, wavelength = .diagnostic)}))\n##\n## #######################################################   \n## save the results\nsave_as(.object = clay_cr_quantiles, .file_path  = processed_spectra_directory, .name= 'clay_cr_quantiles')\nsave_as(.object = clay_reference_spectra, .file_path  = processed_spectra_directory, .name= 'clay_reference_spectra')\nsave_as(.object = diagnostic_clay, .file_path  = processed_spectra_directory, .name= 'diagnostic_clay')\n##\n## tidy up\nrm(list = c('clay_cr_quantiles', 'clay_reference_spectra', 'diagnostic_clay', 'clay_reference_arguments', 'clay_quantiles_cr_arguments' ))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-spectra-quantiles-clay,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6.5,fig.height=3.25,cache = T, background = 1;1;1, warning = F, dependson = setup-libraries;organize-spectra-quantiles-clay ,results=hide,message=F>>=\n## ########################################################\n## figure comparing the continuum removed spectra        ##\n## with the reference spectra                            ##\n## ########################################################\n##\n## load  the required data\nclay_mineral_quantiles <- load_as( .file_path  = processed_spectra_directory, .name= 'clay_cr_quantiles')\nclay_mineral_reference <- load_as( .file_path  = processed_spectra_directory, .name= 'clay_reference_spectra')\nclay_mineral_diagnostics <- load_as( .file_path  = processed_spectra_directory, .name= 'diagnostic_clay')\n## \n## create the plot\nggplot(clay_mineral_quantiles,aes(x = wavelength,y = 1-value)) + \n  geom_line(aes(colour = variable)) + \n  geom_line(data = clay_mineral_reference, colour = 'red') + \n  facet_wrap(~ mineral ,nrow = 1, scales = 'free') +\n  geom_vline(data = clay_mineral_diagnostics, aes( xintercept = wavelength)) + \n  geom_vline(data = clay_mineral_diagnostics, aes( xintercept = (wavelength - 15)), linetype = 2) +\n  geom_vline(data = clay_mineral_diagnostics, aes( xintercept = (wavelength + 15)), linetype = 2) +\n  ylab('Continuum-removed reflectance')+\n  xlab('Wavelength / nm') + \n  scale_x_continuous(breaks = seq(1900, 2050, by=150)) +\n  scale_y_continuous( formatter = 'drop0trailing.format', expand= c(0.01,0)) +\n  scale_colour_brewer('Soil spectra \\n (Quantiles)', palette = 'Greys') +\n  opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), \n       panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), \n       strip.background = theme_blank(),strip.text.x = theme_blank())\n##\n## add the a, b , c labels\ngrid.text(x = 0.05, y = 0.97, label = '(a)')\ngrid.text(x = 0.34, y = 0.97, label = '(b)')\ngrid.text(x = 0.62, y = 0.97, label = '(c)')\n##\n## ########################################################\n## clean up\nrm(list = c('clay_mineral_diagnostics', 'clay_mineral_quantiles', 'clay_mineral_reference'))\n##\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Continuum-removed reflectance for diagnostic wavelengths of various clay minerals: (a) Illite, (b) Kaolinite, (c) Smectite. The red lines are the continuum-removed reflectance for each of the reference clay minerals while the vertical black lines are the diagnostic wavelengths used by \\citet{ViscarraRossel2011}, with the dotted vertical lines indicating the 30-nm band where the local minimum will identify the feature.}\\label{fig:claymineralogy}\n\\end{figure}\n\nThe relative abundances of smectite, kaolinite and illite were calculated by dividing the value at the diagnostic absorbtion band by the corresponding value of the reference mineral spectra. \n The reference spectra came from the USGS Digital Spectral Library \\citep{Clark2007} using samples described in \\citet{Clark1990}. \n Figure~\\ref{fig:claymineralogy} shows that the diagnostic absorbtion bands may be not exactly as defined, instead we use the wavelength corresponding to the local minimum within a 30-nm band around the diagonstic band. \n \n Figures~\\ref{fig:claymineralogy} \\&~\\ref{fig:diagnosticClay} show the absorbtion features clearly for smectite and illite, although the Iilite peak appears to be shifted to nearer 2350 in the legacy spectra. There is little evidence of the kaolinite feature in the sampled spectra.\n Given these results. for Smectite we will compare the deviation from the continuum  from the sampled spectra at 1911 nm with the value from the reference spectra at 1909 nm while for Illite we will compare the value at 2350 nm (sampled) with that at 2346 nm for the reference spectra. \n For kaolinite, we will use the reference spectrum value at 2162 nm and compare it with the value at 2165 nm in the sample spectra.\n \n<<functions-mineralogy-abundance, echo = F, cache = T, results = hide>>=\n## ########################################################\n## some functions for identifying local minima and       ##\n## maxima as part of  mineral abundance calculations     ##\n## using spectra                                         ##\n## ########################################################\nto_band_range <- function(.variable_name, .width,...){\n  ##  function to go from a band range\n  ## to a wide band range\n  value <- as.numeric(list(...)[.variable_name])\n  .between <- value + c(-.width/2,.width/2)\n  }\n## ########################################################\nidentify_local_minima <- function(value, wavelength, .between, ...){\n  ## a function for identify a local minmum between\n  ## two wave bands\n  ##\n  ## which wavelengths are are the wide band\n  .in_interval <- in_interval(.all = wavelength, .interval= .between[1]:.between[2])\n  ##\n  ## where is the mimnum values\n  diagnostic_id  <- which.min(1-value[.in_interval])\n  ##\n  ## what is the minimum value\n  minimum <- value[.in_interval][diagnostic_id]  \n  ##\n  ## what band is the mininum value at\n  diagnostic_band  <- wavelength[.in_interval][diagnostic_id]\n  ###\n  ## return a data frame\n  data.frame(diagnostic_band, diagnostic_id, minimum, in_range_min = min(.between), \n            in_range_max = max(.between), using = 'local_minimum')  \n  ##\n  }\n## ########################################################\nidentify_deriv_minima <- function(value, wavelength, .between, n= 11, p = 2,...){\n  ## a function for identify a local minmum between\n  ## two wave bands using derivatives\n  ##\n  ## which wavelengths are in the interval?\n  .in_interval <- in_interval(.all = wavelength, .interval= .between[1]:.between[2])\n  ##\n  ## get the derivative \n  derivative <- sgolayfilt(value, n = n, p = p, m = 1)\n  ##\n  ## where is the derivative zeo\n  diagnostic_id <- which.max(cumsum(derivative[.in_interval] >0))\n  ##\n  ## to which wavelength does that correspond?\n  diagnostic_band <- wavelength[.in_interval][diagnostic_id]\n  ##\n  ## what is the minumum value\n  minimum <- value[.in_interval][diagnostic_id]\n  ##\n  ## return the information\n  data.frame(diagnostic_band, diagnostic_id, minimum, in_range_min = min(.between),\n              in_range_max = max(.between), using = 'deriv_1')\n  ##\n  }\n## ########################################################\nspectral_minima_multiple <- function(.spectra_path, .spectra_name, .type, .id,.between, .interval, n = 11, p = 2, ...){\n  ## a function to read in spectra from a file\n  ## and calculate the mininum value and\n  ## corresponding wavelenth using derivatives \n  ## and local minima\n  ##\n  ## load spectra \n  spectra <- load_as(.file_path = .spectra_path, .name = .spectra_name)\n  ##\n  ## find the minima using derivatives\n  results_deriv <- adply(spectra, 1, identify_deriv_minima, wavelength = .interval, .between = .between, n = n, p = p)\n  ##\n  ## find the minima using raw values\n  results_local <-adply(spectra, 1, identify_local_minima, wavelength = .interval, .between = .between)\n  ##\n  ## return the results\n  rbind(results_deriv,results_local)\n  ##\n  }\n## ########################################################\nrelative_abundance <- function(minimum, reference_value, ...){\n  ## a function to calculate the relative abundance given\n  ## the minimum value and reference value\n  ##\n  minimum / reference_value\n  }\n## ########################################################\nnormalized_difference <- function(A,B){\n  ## calculate normalize difference\n  ab <- cbind(A,B)\n  ret <- (A-B)/ (A+B)\n  ##look for NA (dividing by 0)\n  both0 <- apply(ab,1, function(x){(x[1] == 0)&(x[2]==0)})\n  ## set those to 0\n  ret[both0] <- rep(0, table(both0)[2])\n  ## return\n  ret\n  ##\n  }\n## ########################################################\n@\n \n<<analysis-mineralogy-clay-abundance, cache = T, echo = F, results = hide, dependson = functions-mineralogy-abundance; organize-spectra-quantiles-clay>>=\n## #########################################################\n## calculations for mineral abundance                     ##\n## clay minerals                                          ##\n## #########################################################\n##\n## load the required data\nclay_mineral_diagnostics <- load_as(.file_path  = processed_spectra_directory, .name = 'diagnostic_clay')\nclay_mineral_reference <- load_as(.file_path  = processed_spectra_directory, .name = 'clay_reference_spectra')\n##\n## create the extended band widths\nclay_diagnostic_wideband <- dlply(clay_mineral_diagnostics, .(mineral), splat(to_band_range), .width = 30, .variable_name = 'wavelength')\n##\n## create the the argument list for identifying the local mimimum within the band range for the reference spectra\nclay_reference_diagnostic_list <- dlply(clay_mineral_reference, .(.mineral), combine_list , \n  .which_x = c('value','wavelength'), .y = clay_diagnostic_wideband, .which_y = '.between', .id.var = 'mineral')\n##\n## identify the location of the diagostic features and the value in the reference spectra\nclay_reference_diagnostic_results <- ldply(clay_reference_diagnostic_list, .fun = splat(identify_local_minima))\n##\n## ########################################################\n## now for the sample spectra\n##\n## create the argument list\nclay_spectra_diagnostic_list <- llply(dlply( join(clay_minerals_cr_files, clay_mineral_diagnostics), .(.id), combine_list,.which_x = c('.spectra_path', '.spectra_name','.id'), .y = clay_diagnostic_wideband, .which_y = '.between', .id.var = '.id'), combine_list, .which_x = c('.spectra_path', '.spectra_name', '.id','.between'),  .y = filter_cr_clay_arguments,   .which_y = '.interval',.id.var = '.id')                             \n##\n## identify using local minimum and derivatives\nclay_diagnostic_results <- ldply(clay_spectra_diagnostic_list, splat(spectral_minima_multiple))\n##\n## ########################################################\n## save \nsave_as(.object = clay_reference_diagnostic_results, .file_path = processed_spectra_directory, .name = 'clay_reference_diagnostic_results')\nsave_as(.object = clay_diagnostic_wideband, .file_path = processed_spectra_directory, .name = 'clay_diagnostic_wideband')\nsave_as(.object = clay_diagnostic_results, .file_path = processed_spectra_directory, .name = 'clay_diagnostic_results')\n##\n## clean up\nrm(list = c('clay_reference_diagnostic_results', 'clay_diagnostic_wideband', 'clay_diagnostic_results', 'spectra_diagnostic_list', 'reference_diagnostic_list'))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-clay-mineralogy-diagnostic-bands,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6.5, fig.height = 3.25, cache = T, background = 1;1;1, warning = F, dependson = setup-libraries;analysis-mineralogy-clay-abundance, results=hide, message=F>>=\n## ########################################################\n## a figure showing the location of diagnostic wavebands ##\n## for various clay minerals                             ##\n## ########################################################\n##\n## load required data \nclay_mineral_diagnostics <- load_as(.file_path  = processed_spectra_directory, .name = 'diagnostic_clay')\nclay_diagnostic_results <- load_as(.file_path = processed_spectra_directory, .name = 'clay_diagnostic_results')\nclay_reference_diagnostic_results  <- load_as(.file_path = processed_spectra_directory, .name = 'clay_reference_diagnostic_results')\n##\n## create plot\nggplot(clay_diagnostic_results, aes(x= diagnostic_band, colour = using))  +\n  geom_density() + \n  facet_wrap(~.id, scales='free', ncol = 3)+\n  ylab('Density') + \n  xlab('Wavelength') +\n   geom_vline(data = clay_mineral_diagnostics ,aes( xintercept = wavelength), colour ='red') +\n   geom_vline(data = clay_reference_diagnostic_results ,aes( xintercept = diagnostic_band), colour ='blue') +\n   scale_colour_manual('Method', breaks = c('deriv_1','local_minimum'), labels = c('Derivative','Local minimum'), values = c('black','darkgrey'))+\n   opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank(),strip.text.x = theme_blank())\n##\n## add the a, b , c labels\ngrid.text(x = 0.05, y = 0.97, label = '(a)')\ngrid.text(x = 0.32, y = 0.97, label = '(b)')\ngrid.text(x = 0.59, y = 0.97, label = '(c)')\n##\n## tidy up\nrm( list = c('clay_mineral_diagnostics', 'clay_diagnostic_results', 'clay_reference_diagnostic_results'))\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Density plots of the location of diagnostic features for (a) smectite, (b) kaolinite and (c) illite. The red vertical line is the location of the feature on the reference spectra and the blue line the location accordining to \\citet{Clark1990}. }\\label{fig:diagnosticClay}\n\\end{figure}\n\n<<analysis-mineralogy-clay-relative-abundance, echo =F, cache = T, results = hide,dependson =mineral-abundance >>=\n## ########################################################\n## calculate the relative abundance of the different     ##\n## clay minerals                                         ##\n## ########################################################\n##\n## load the required data\nclay_diagnostic_results <- load_as(.file_path = processed_spectra_directory, .name = 'clay_diagnostic_results')\nclay_reference_diagnostic_results  <- load_as(.file_path = processed_spectra_directory, .name = 'clay_reference_diagnostic_results')\n##\n## caculate the relative abundances\n##\n## organize the data and arguments\nnames(clay_diagnostic_results)[1] <- '.mineral'\n##\n## create argument list\nclay_abundance_arguments <- merge(clay_diagnostic_results,ddply(clay_reference_diagnostic_results, .(.mineral), splat(function( minimum, ...){data.frame( reference_value = minimum)}) ))\n##\n## calculate the relative abundances\nclay_diagnostic_results$relative_abundance <-  splat(relative_abundance)(clay_abundance_arguments)\n##\n## save results\nsave_as(.object = clay_diagnostic_results, .file_path = processed_spectra_directory, .name = 'relative_abundance_clay')\n## \n## clean up\nrm(list = c('clay_diagnostic_results','clay_reference_diagnostic_results', 'clay_abundance_arguments' ))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-clay-mineralogy-relative-abundance,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6.5, fig.height = 3.25, cache = T, background = 1;1;1, warning = F, dependson = setup-libraries;mineral-abundance, results=hide, message=F>>=\n## ########################################################\n## create a plot of the relative abundance distributions ##\n## for the clay minerals                                 ##\n## ########################################################\n##\n## load required data\nclay_relative_abundance <- load_as(.file_path = processed_spectra_directory, .name = 'relative_abundance_clay')\n##\n## create the ploy\nggplot(clay_relative_abundance)+\n  geom_density(aes(x= relative_abundance, colour= using)) + \n  facet_wrap(~.mineral,scales='free', nrow=1)+\n  ylab('Density') + \n  xlab('Abundance') +\n  scale_colour_manual('Method', breaks = c('deriv_1','local_minimum'), labels = c('Derivative','Local minimum'), values = c('black','darkgrey')) +\n  opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank(),strip.text.x = theme_blank())\n##\n## add the a, b , c labels\ngrid.text(x = 0.05, y = 0.97, label = '(a)')\ngrid.text(x = 0.32, y = 0.97, label = '(b)')\ngrid.text(x = 0.59, y = 0.97, label = '(c)')\n##\n## clean up\nrm(clay_relative_abundance)\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Density plots of the relative abundance (a) illite, (b) kaolinite and (c) smectite caculated on the sample spectra. }\\label{fig:abundance:clay}\n\\end{figure}\n\n\n\\section{Iron oxide mineralogy} \\label{section:Fe}\nAbsorbtion due to hematite and goethite occure near 880 nm and 920 nm \\citep{Clark1990}. \nPrevious work has found the identifying thes presence of these Fe oxides was difficult due to shifts in the exact location, presence in small amounts   and masking by a  broad absorbtion due to organic matter at 650 nm \\citep{ViscarraRossel2010}. \n To account for this possible shift, they located the local minimum reflectance in $[860 - 930]$ for hematite and $[910 - 970]$ for goethite.\n Once the locations of the local minima are  found, the absorption band depth ($D_{\\lambda}$) for the particular wavelength $\\lambda$ was calculated by\n \\begin{equation}\\label{eq:banddepth}\n D_{\\lambda} = 1 - CR\n \\end{equation}\nwhere CR is the continuum-removed reflectance value.\n As an alternative, we use the wavelength and value associated with the maximum absorbtion band depth in the two wavelength ranges. \n\n\nWe use these absorption depths for hematite and goethite to calculate a normalized  Fe Oxide difference index (NIODI)  \\citep{ViscarraRossel2010}\n \\begin{equation}\\label{eq:NIODI}\n NIODI = \\frac{D_{920} - D_{880}}{D_{920}+D_{880}}\n \\end{equation}\n \n<<organize-fe-oxide-mineralogy, echo = F, results = hide, cache = T>>=\n## ########################################################\n## set up for iron oxide abundance calculations          ##\n## set wavelength intervals for hematite and goethite    ##\n## ########################################################\n##\n## set bandwidth information for iron oxides fo\nfe_minerals_list  <- list(hematite = list(.interval = 860:930, mineral = 'hematite', .diagnostic = 880), goethite = list(.interval = 910:970, mineral = 'goethite', .diagnostic = 920))\n##\n## ########################################################\n@\n\n<<readin-fe-oxide-reference-spectra, cache = T, echo = F, results = hide, dependson = continuum-removal; functions-mineralogy>>=\n## ########################################################\n## read in the reference spectra for iron oxide minerals ##\n## from USGS spectra                                     ##\n## process using s-g filter                              ##\n## ########################################################\nfe_reference_list <- list(hematite = list( .spectra_fname = 'hematite_fe2602.9271.asc', mineral = 'hematite',       .type = 'reflect'), goethite = list(.spectra_fname = 'goethite_ws220.8392.asc', mineral = 'goethite', .type= 'reflect'))\n##\n## read in spectra and save\nfe_reference <- ldply(fe_reference_list, splat(read_reference_spectra), .all = wavelengths,                       .save_path = raw_spectra_directory)\n##\n## process with sg-filter\nfe_reference_sg <- ddply(fe_reference, .(mineral), splat(filter_sg), n = 11, p = 2, m = 0,                           .save_path =  processed_spectra_directory, .spectra_ext = '.RData')\n##\n## ########################################################\n@\n\n \n<<analysis-mineralogy-fe-oxide-continuum-removal, cache = T, results = hide, echo = F,dependson =organize-fe-oxide-mineralogy; functions-filter-continuum-removal>>=\n## ########################################################\n## Calculate the continuum removed spectra in each       ##\n## waveband interval for the iron oxide minerals         ##\n## ########################################################\n## organize the filter arguments as a list   \nfilter_cr_fe_arguments <- llply(fe_minerals_list, function(x,y){c(x,as.list(y))}, y = filter_sg_files[1,])   \n##\n## run the continuum removal\nfe_minerals_cr_files <-ldply(filter_cr_fe_arguments, splat(filter_continuum_removal), .save_path = processed_spectra_directory, .all = wavelengths)\n##\n## ########################################################\n## Continuum removal for reference spectra for the       ##\n## clay minerals                                         ##  \n## ########################################################\n## \n## organize the arguments as a list\nfe_reference_cr_list <-  alply( fe_reference_sg, 1, combine_list, .which_x = names(fe_reference_sg[,-1]), .y = fe_minerals_list, .which_y = names(fe_minerals_list[[1]]), .id.var = 'mineral')\n## \n## run the continuum-removal \nfe_reference_cr_files <-ldply(fe_reference_cr_list, splat(filter_continuum_removal), .save_path = processed_spectra_directory, .all = wavelengths)\n##\n## ########################################################\n@\n\n<<organize-spectra-quantiles-fe-oxide, echo = F, cache = T, dependson = analysis-mineralogy-fe-oxide-continuum-removal>>=\n## ########################################################\n## create quantiles of the continuum-removed spectra     ##\n## for plotting against the reference spectra            ##\n## Iron oxides                                           ##\n## ########################################################\n##\n## organize the arguments\nfe_quantiles_cr_arguments <-  alply( fe_minerals_cr_files, 1, combine_list, .which_x = names(fe_reference_sg[,-1]), .y = fe_minerals_list, .which_y = names(fe_minerals_list[[1]]), .id.var = 'mineral')\n##\n## get the quantiles of the cr-spectra for the samples\nfe_cr_quantiles <- ldply(fe_quantiles_cr_arguments, splat(spectral_quantiles), probs =c(0.05,0.06, 0.5,0.84, 0.95),  .save_path = processed_spectra_directory)\n##\n## ########################################################\n## get the reference spectra in the same format   \n##\n## an argument list\nfe_reference_arguments <-  alply( fe_reference_cr_files, 1, combine_list, .which_x = names(fe_reference_sg[,-1]), .y = fe_minerals_list, .which_y = names(fe_minerals_list[[1]]), .id.var = 'mineral')\n##\n## and reading and organizing\nfe_reference_spectra <- ldply( fe_reference_arguments, splat (function(.spectra_path, .spectra_name, .interval, mineral, ...){ data.frame(value = load_as(.file_path = .spectra_path, .name = .spectra_name), wavelength = .interval,.mineral = mineral)}))\n##\n## ########################################################\n## and the diagnostic bands\n##\n## set up diagnostic bands as a data frame\nfe_diagnostic <- ldply(fe_minerals_list,splat(function(mineral,.diagnostic,...){data.frame(mineral, wavelength = .diagnostic)}))\n##\n## #######################################################   \n## save the results\nsave_as(.object = fe_cr_quantiles, .file_path  = processed_spectra_directory, .name= 'fe_cr_quantiles')\nsave_as(.object = fe_reference_spectra, .file_path  = processed_spectra_directory, .name= 'fe_reference_spectra')\nsave_as(.object = fe_diagnostic, .file_path  = processed_spectra_directory, .name= 'fe_diagnostic')\n##\n## tidy up\nrm(list = c('fe_cr_quantiles', 'fe_reference_spectra', 'fe_diagnostic', 'fe_reference_arguments', 'fe_quantiles_cr_arguments' ))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-spectra-quantiles-fe,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6.5,fig.height=3.25,cache = T, background = 1;1;1, warning = F, dependson = setup-libraries;organize-spectra-quantiles-fe ,results=hide,message=F>>=\n## ########################################################\n## figure comparing the continuum removed spectra        ##\n## with the reference spectra                            ##\n## ########################################################\n##\n## load  the required data\nfe_mineral_quantiles <- load_as( .file_path  = processed_spectra_directory, .name= 'fe_cr_quantiles')\nfe_mineral_reference <- load_as( .file_path  = processed_spectra_directory, .name= 'fe_reference_spectra')\nfe_mineral_diagnostics <- load_as( .file_path  = processed_spectra_directory, .name= 'fe_diagnostic')\n## \n## create the plot\nggplot(fe_mineral_quantiles,aes(x = wavelength,y = 1-value)) + \n  geom_line(aes(colour = variable)) + \n  geom_line(data = fe_mineral_reference, colour = 'red') + \n  facet_wrap(~ mineral ,nrow = 1, scales = 'free') +\n  geom_vline(data = fe_mineral_diagnostics, aes( xintercept = wavelength)) + \n  ylab('Continuum-removed reflectance')+\n  xlab('Wavelength / nm') + \n  scale_y_continuous( formatter = 'drop0trailing.format', expand= c(0.01,0)) +\n  scale_colour_brewer('Soil spectra \\n (Quantiles)', palette = 'Greys') +\n  opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), \n       panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), \n       strip.background = theme_blank(),strip.text.x = theme_blank())\n##\n## add the a, b , c labels\ngrid.text(x = 0.05, y = 0.97, label = '(a)')\ngrid.text(x = 0.49, y = 0.97, label = '(b)')\n##\n## ########################################################\n## clean up\nrm(list = c('fe_mineral_diagnostics', 'fe_mineral_quantiles', 'fe_mineral_reference'))\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Continuum-removed reflectance for diagnostic wavelengths of various Iron oxide minerals: (a) Hematice, (b) Goethite. The red lines are the continuum-removed reflectance for each of the reference iron oxide minerals while the vertical black lines are the diagnostic wavelengths used by \\citet{ViscarraRossel2011}. }\\label{fig:fe-oxidemineralogy}\n\\end{figure}\n\n\nFigure~\\ref{fig:fe-oxidemineralogy} shows the continuum-removed reflectance in the wavelength bands for hematite and goethite. \n\n<<analysis-mineralogy-fe-oxide-abundance, cache = T, echo = F, results = hide, dependson = functions-mineralogy-abundance; organize-spectra-quantiles-clay>>=\n## #########################################################\n## calculations for mineral abundance                     ##\n## iron oxide minerals                                    ##\n## #########################################################\n##\n## load the required data\nfe_mineral_diagnostics <- load_as(.file_path  = processed_spectra_directory, .name = 'fe_diagnostic')\nfe_mineral_reference <- load_as(.file_path  = processed_spectra_directory, .name = 'fe_reference_spectra')\n##\n## create the extended band widths -- note that these are not symmetric\nfe_diagnostic_wideband <- llply(fe_minerals_list, splat(function(.interval,...){data.frame(.between = range(.interval))}))\n##\n## create the the argument list for identifying the local mimimum within the band range for the reference spectra\nfe_reference_diagnostic_list <- dlply(fe_mineral_reference, .(.mineral), combine_list , \n  .which_x = c('value','wavelength'), .y = fe_diagnostic_wideband, .which_y = '.between', .id.var = 'mineral')\n##\n## identify the location of the diagostic features and the value in the reference spectra\nfe_reference_diagnostic_results <- ldply(fe_reference_diagnostic_list, .fun = splat(identify_local_minima))\n##\n## ########################################################\n## now for the sample spectra\n##\n## create the argument list\nfe_spectra_diagnostic_list <- llply(dlply( join(fe_minerals_cr_files, fe_mineral_diagnostics), .(.id), combine_list,.which_x = c('.spectra_path', '.spectra_name','.id'), .y = fe_diagnostic_wideband, .which_y = '.between', .id.var = '.id'), combine_list, .which_x = c('.spectra_path', '.spectra_name', '.id','.between'),  .y = filter_cr_fe_arguments,   .which_y = '.interval',.id.var = '.id')                             \n##\n## identify using local minimum and derivatives\nfe_diagnostic_results <- ldply(fe_spectra_diagnostic_list, splat(spectral_minima_multiple))\n##\n## ########################################################\n## save \nsave_as(.object = fe_reference_diagnostic_results, .file_path = processed_spectra_directory, .name = 'fe_reference_diagnostic_results')\nsave_as(.object = fe_diagnostic_wideband, .file_path = processed_spectra_directory, .name = 'fe_diagnostic_wideband')\nsave_as(.object = fe_diagnostic_results, .file_path = processed_spectra_directory, .name = 'fe_diagnostic_results')\n##\n## clean up\nrm(list = c('fe_reference_diagnostic_results', 'fe_diagnostic_wideband', 'fe_diagnostic_results', 'fe_spectra_diagnostic_list', 'fe_reference_diagnostic_list'))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-fe-mineralogy-diagnostic-bands,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6.5, fig.height = 3.25, cache = T, background = 1;1;1, warning = F, dependson = setup-libraries;analysis-mineralogy-fe-abundance, results=hide, message=F>>=\n## ########################################################\n## a figure showing the location of diagnostic wavebands ##\n## for various fe minerals                             ##\n## ########################################################\n##\n## load required data \nfe_mineral_diagnostics <- load_as(.file_path  = processed_spectra_directory, .name = 'fe_diagnostic')\nfe_diagnostic_results <- load_as(.file_path = processed_spectra_directory, .name = 'fe_diagnostic_results')\nfe_reference_diagnostic_results  <- load_as(.file_path = processed_spectra_directory, .name = 'fe_reference_diagnostic_results')\n##\n## make everything uniform in type and names\nfe_reference_diagnostic_results$.id <- as.character(fe_reference_diagnostic_results$.mineral)\n##\n## create plot\nggplot(fe_diagnostic_results, aes(x= diagnostic_band, colour = using))  +\n  geom_density() + \n  ylab('Density') + \n  xlab('Wavelength') +\n   geom_vline(data = fe_mineral_diagnostics ,aes( xintercept = wavelength), colour ='red') +\n   geom_vline(data = fe_reference_diagnostic_results ,aes( xintercept = diagnostic_band), colour ='blue') +\n   scale_colour_manual('Method', breaks = c('deriv_1','local_minimum'), labels = c('Derivative','Local minimum'), values = c('black','darkgrey'))+\n   facet_wrap(~.id, scales='free', ncol = 2)+\n   opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank(),strip.text.x = theme_blank())\n##\n## add the a, b , c labels\ngrid.text(x = 0.05, y = 0.97, label = '(a)')\ngrid.text(x = 0.45, y = 0.97, label = '(b)')\n##\n## tidy up\nrm( list = c('fe_mineral_diagnostics', 'fe_diagnostic_results', 'fe_reference_diagnostic_results'))\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Density plots of the location of diagnostic features for (a) hematite, (b) goethite. The blue vertical line is the location of the feature on the reference spectra and the red line the location accordining to \\citet{Clark1990}. }\\label{fig:diagnostic-feoxide}\n\\end{figure}\n\n<<analysis-mineralogy-fe-oxide-relative-abundance, echo =F, cache = T, results = hide,dependson =mineral-abundance >>=\n## ########################################################\n## calculate the relative abundance of the different     ##\n## iron oxide minerals                                   ##\n## ########################################################\n##\n## load the required data\nfe_diagnostic_results <- load_as(.file_path = processed_spectra_directory, .name = 'fe_diagnostic_results')\nfe_reference_diagnostic_results  <- load_as(.file_path = processed_spectra_directory, .name = 'fe_reference_diagnostic_results')\n##\n## caculate the relative abundances\n##\n## organize the data and arguments\nnames(fe_diagnostic_results)[1] <- '.mineral'\n##\n## create argument list\nfe_abundance_arguments <- merge(fe_diagnostic_results,ddply(fe_reference_diagnostic_results, .(.mineral), splat(function( minimum, ...){data.frame( reference_value = minimum)}) ))\n##\n## calculate the relative abundances\nfe_diagnostic_results$relative_abundance <-  splat(relative_abundance)(fe_abundance_arguments)\n##\n## save results\nsave_as(.object = fe_diagnostic_results, .file_path = processed_spectra_directory, .name = 'relative_abundance_fe')\n## \n## clean up\nrm(list = c('fe_diagnostic_results','fe_reference_diagnostic_results', 'fe_abundance_arguments' ))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-fe-oxide-mineralogy-relative-abundance,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6.5, fig.height = 3.25, cache = T, background = 1;1;1, warning = F, dependson = setup-libraries;mineral-abundance, results=hide, message=F>>=\n## ########################################################\n## create a plot of the relative abundance distributions ##\n## for the iron oxide minerals                           ##\n## ########################################################\n##\n## load required data\nfe_relative_abundance <- load_as(.file_path = processed_spectra_directory, .name = 'relative_abundance_fe')\n##\n## create the plot\nggplot(fe_relative_abundance)+\n  geom_density(aes(x= relative_abundance, colour= using)) + \n  facet_wrap(~.mineral,scales='free', nrow=1)+\n  ylab('Density') + \n  xlab('Abundance') +\n  scale_colour_manual('Method', breaks = c('deriv_1','local_minimum'), labels = c('Derivative','Local minimum'), values = c('black','darkgrey')) +\n  opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank(),strip.text.x = theme_blank())\n##\n## add the a, b , c labels\ngrid.text(x = 0.04, y = 0.97, label = '(a)')\ngrid.text(x = 0.45, y = 0.97, label = '(b)')\n##\n## clean up\nrm(fe_relative_abundance)\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Density plots of the relative abundance (a) hematite and (b) goethite (c) smectite caculated on the sample spectra. }\\label{fig:abundance:fe}\n\\end{figure}\n\n<<analysis-fe-oxide-normalized-difference, echo = T, cache = T, results = hide, dependson =functions-mineralogy-abundance;nalysis-mineralogy-fe-oxide-continuum-removal;analysis-mineralogy-fe-oxide-relative-abundance>>=\n## ########################################################\n## calculate the difference index for                    ##\n## hematite and goethite                                 ##\n## ########################################################\n##\n## load data and cast in form useful for the normalized difference\nfe_minima <- cast(melt(load_as(.file_path = processed_spectra_directory, .name = 'fe_diagnostic_results'), id.var = c('.id','X1','using'), measure.var = 'minimum'),X1~.id+using)\n##\n## do the calculations for the local- derivative-derived minima\nnd_fe_local <- with(fe_minima, normalized_difference(A = goethite_deriv_1, B = hematite_deriv_1))\nnd_fe_deriv <- with(fe_minima, normalized_difference(A = goethite_deriv_1, B = hematite_deriv_1))\n##\n## create single data frame\nndi_fe_data <- reshape2:::melt.data.frame(data.frame(local_minimum = nd_fe_local, deriv_1 =nd_fe_deriv ), measure.vars = c('local_minimum','deriv_1'), variable.name = 'method', value.name = 'ndi_fe')\n##\n## save results\nsave_as(.object = ndi_fe_data, .file_path = processed_spectra_directory, .name = 'ndi_fe_data')\n##\n## clean up\nrm(list = c('niodi_data','nd_fe_local','nd_fe_deriv','fe_minima'))\ninvisible(gc())\n## ########################################################\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-fe-oxide-difference-index,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6.5, fig.height = 3.25, cache = T, background = 1;1;1, warning = F, dependson = setup-libraries;mineral-abundance, results=hide, message=F>>=\n## ########################################################\n## create a plot of the normalized difference index      ##\n## for the iron oxide minerals                           ##\n## ########################################################\n##\n## load required data\nndi_fe_data <- load_as(.file_path = processed_spectra_directory, .name = 'ndi_fe_data')\n##\n## create the plot\nggplot(ndi_fe_data)+\n  geom_density(aes(x=ndi_fe, colour= method)) + \n  ylab('Density') + \n  xlab('Normalized Iron oxide difference index') +\n  scale_colour_manual('Method', breaks = c('deriv_1','local_minimum'), labels = c('Derivative','Local minimum'), values = c('black','darkgrey')) +\n  opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank(),strip.text.x = theme_blank())\n##\n## clean up\nrm(ndi_fe_data)\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Density plots of the Normalized Iron oxide difference index derived using two methods. Positive values indicate the presence of goethite, negative values hematite. Values approaching zero indicate either the index cannot determine a dominant mineral.   }\\label{fig:nd-fe}\n\\end{figure}\n\n\n\n\\section{Refined predictive models}\n\\subsection{Removing spectral outliers}\n\\subsubsection{Visual inspection of raw spectra}\n\n<<organize-identify-spectral-outliers, echo = F, cache = T, results = hide, dependson = functions-basic; readin-soildata; readin-spectra>>=\n## ########################################################\n## inspecting the raw spectra showed that two samples    ##\n## were outliers or had errors when recorded             ##\n## Here, create a data frame to show these problematic   ##\n## spectra    c(1981,3144)                               ##\n## ########################################################\n## \n## set up outlier identifiers\nvisual_outliers <- c(1981,3144)\n##\n## get their identifiers\nvisual_outliers_id <- as.character(soil_data$user_horizon_id[visual_outliers])\n##\n## ########################################################\n## load and arrange outlier spectra\n## get arguments\noutlier_arguments <- merge(raw_spectra_files, data.frame( .which =  c(visual_outliers,visual_outliers + 1), .is_outlier = rep(c('outlier','regular'), each = 2), .group = rep(visual_outliers_id,2)) )\n##\n##create data.frame\noutlier_spectra <- adply(outlier_arguments,1, splat(load_single_spectra), wavelengths = wavelengths)\n##\n## ########################################################\n## remove from soil sample data\n##\nsoil_data_v <- soil_data[-visual_outliers,] \n##\n## add row id\nsoil_data_v$row_index <- as.numeric(row.names(soil_data_v))\n##\n## ########################################################\n## save objects\nsave_as(.object = outlier_spectra, .file_path = raw_spectra_directory, .name = 'outlier_spectra')\nsave_as(.object = soil_data_v, .file_path = saved_data_directory, .name = 'soil_data_no_visual_outliers')\nsave_as(.object = visual_outliers_id,  .file_path = saved_data_directory, .name = 'visual_outliers_id')\n##\n##\n\n## clean up\nrm(c('outlier_arguments','outlier_spectra', 'soil_data_v'))\ninvisible(gc())\n##\n## ########################################################\n@\n\nOne sample [\\Sexpr{load_as( .file_path = saved_data_directory, .name = 'visual_outliers_id')[1] }] was removed due as an spectral outlier or an error in recording. \n Another sample [\\Sexpr{load_as( .file_path = saved_data_directory, .name = 'visual_outliers_id')[2] }]  was removed as the spectrum recorded was a horizontal line at reflectance = 1.\n Figure~\\ref{fig:spectral-outlier} shows this spectrum with a comparison spectrum from a lower horizon in the same profile.\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-spectral-outlier,cache = T, fig = T, dev = tikz, fig.height = 3, fig.width = 6, out.width = 0.9\\textwidth, results = hide, echo = F, background = 1;1;1, dependson = organize-identify-spectral-outliers>>=\n## ########################################################\n## create the plot of the outlier spectra                ##\n## with comparison spectra                               ##\n## ########################################################\n##\n## load required data\noutlier_spectra <- load_as(.file_path = raw_spectra_directory, .name = 'outlier_spectra')\n## \n## create nice labels\nlevels(outlier_spectra$.type) <- c('$R$', '$ - {\\\\rm log} ( R)$')\n##\n## create plot\nggplot(outlier_spectra,aes(y=value, x =wavelengths, colour = .is_outlier, group = factor(.which)))+ \n  geom_line() + \n  facet_grid(.type~ .group,scales='free') +\n  scale_colour_manual('Sample \\n type', values = c('red','black')) +\n  xlab('Wavelengths / nm') +\n  ylab('Value')+\n  opts(legend.title = theme_text(  size = 12 * 0.8, hjust = 0, face = 'plain'), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank())\n##\n## clean up\nrm(outlier_spectra)\ninvisible(gc())\n##\n## ########################################################\n@\n\\caption{Outlier (red) and comparison spectra  (black).}\\label{fig:spectral-outlier}\n\\end{figure}\n\n\\subsubsection{Statistical methods for multivariate outliers}\n\nThe Mahalanobis distance can also be used to assess whether  a sample is a potential outlier. \n The   Mahalanobis distance was calculated on the scores of the first five PLS factors. The rates of outlier detection using normal and robust estimates are shown in table~\\ref{table:mvOutliers}\n \n<<functions-outlier-detection, cache = T, echo = F, results = hide, dependson = functions-basic;organize-identify-spectral-outliers>>=\n## ########################################################\n## functions to detect outliers using mahalanobis         ##\n## distance                                              ##\n###########################################################\nmahalanobis_outlier <- function(value, variable, .id_column, plsr_scores = 5, .spectra_path, .spectra_name, .keep, critical_value, ... ){\n  ## ######################################################\n  ## a function to calculate the mahalanobis distance    ##\n  ## for the first plsr_scores (default = 5) pls scores  ##\n  ## and then use chi squared statistic to identify      ##\n  ## outliers                                           ##\n  ## ######################################################\n  ##\n  ## get the row indexes\n  other_args <- list(...)\n  id_samples <- other_args[[.id_column]]\n  ##\n  ## load spectra -- trimming appropriately\n  spectra <- load_as(.file_path = .spectra_path, .name = .spectra_name)[id_samples,.keep]\n  ##\n  ## organize data\n  pls_data <- data.frame(value, spectra = I(as.matrix(spectra)))\n  ##\n  ## fit the model\n  pls_model <- plsr(value ~spectra, data = pls_data, ncomp = plsr_scores)\n  ##\n  ## get the pls scores scores\n  variable_scores <- scores(pls_model)\n  ##\n  ## calculate Mahalanobis distance on scores and test \n  ## for significance\n  ## uses sign1 from the mvoutlier pacakge\n  outlier_test <- sign1(variable_scores, qcrit = critical_value)\n  ##\n  ## return data frame  showing which samples are outliers\n  potential_outlier <- !as.logical(outlier_test$wfinal01)\n  ##\n  ## return\n  return(data.frame(potential_outlier, row_index = id_samples))\n  ##\n  }\n## ########################################################\n\n@\n<<analysis-mahalanobis-outlier-detection,cache = T, echo = F, results = hide, warning=F; dependson = organize-identify-spectral-outliers;functions-outlier-detection >>=\n## ########################################################\n## identify multivariate spectral (and variable) outliers #\n## using mahalanobis distance                            ##\n## and sg filtered first derivative                      ##\n## ########################################################\n##\n## load required soil property data\n## visual outliers removed!\nsoil_data_no_v_o <- load_as(.file_path = saved_data_directory, .name = 'soil_data_no_visual_outliers')\n##\n## melt to appropriate format (removing NAs )\nmolten_soil_no_v_o <- melt(soil_data_no_v_o, id.vars = c('user_pedon_id', 'lab_project_name','lab_horizon_id','row_index'),measure.vars = which_properties, na.rm=T)\n##\n## reflectance outliers\nreflectance_sgd1_outliers <- ddply(molten_soil_no_v_o, .(variable), splat(mahalanobis_outlier), .id_column = 'row_index', plsr_scores = 5, .spectra_path = filter_sg_sub10_files[2,'.spectra_path'], .spectra_name =filter_sg_sub10_files[2,'.spectra_name'], .keep = 1:206, critical_value = 0.975)\n##\n## absorbance outliers\nabsorbance_sgd1_outliers <- ddply(molten_soil_no_v_o, .(variable), splat(mahalanobis_outlier), .id_column = 'row_index', plsr_scores = 5, .spectra_path = filter_sg_sub10_files[4,'.spectra_path'], .spectra_name =filter_sg_sub10_files[4,'.spectra_name'], .keep = 1:206, critical_value = 0.975)\n##\n## save\nsave_as(.object = molten_soil_no_v_o, .file_path = saved_data_directory, .name = 'molten_soil_no_v_o')\nsave_as(.object = reflectance_sgd1_outliers, .file_path = training_info_directory, .name = 'reflectance_sgd1_outliers')\nsave_as(.object = absorbance_sgd1_outliers, .file_path = training_info_directory, .name = 'absorbance_sgd1_outliers')\n##\n## clean up\nrm(list = c('soil_data_no_v_o', 'molten_soil_no_v_o', 'reflectance_sgd1_outliers', 'absorbance_sgd1_outliers'))\ninvisible(gc())\n##\n## ########################################################\n@\n\n\\begin{table}[tbp]\n\\caption{Proportion of outliers per variable calculated using the Mahalanobis distance}\n\\label{table:mvOutliers}\n\\centering\n<<table-mahalanobis-outliers, echo=F, results = tex, cache = F, dependson = setup-libraries;analysis-mahalanobis-outlier-detection ,warning=F,message=F>>=\n## ########################################################\n## creates a table noting the total number of samples    ##\n## and outliers for each soil variable and spectral type ##\n## format a table using booktabs in LaTeX                ##\n## ########################################################\n##\n## load required data\noutliers_sgd1 <- melt(rbind(data.frame(load_as( .file_path = training_info_directory, .name = 'reflectance_sgd1_outliers'), .type = 'Reflectance'),data.frame(load_as( .file_path = training_info_directory, .name = 'absorbance_sgd1_outliers'), .type = 'Absorbance') ) , .id.vars = c('.type','variable','row_index'))\n##\n## make into a table\noutliers_sgd1_table <- cast(ddply(outliers_sgd1, .(variable, .type), summarise, total = length(value), n_outliers = length(value) *(1- mean(value))),  variable ~ .type, variable = 'n_outliers')\n##\n## add the 'total' column\noutliers_sgd1_table$Total <- cast(ddply(outliers_sgd1, .(variable, .type), summarise, total = length(value), n_outliers = length(value) *(1- mean(value))),  variable ~ .type,value = 'total')$Reflectance\n##\n## set the row names to latexed properties\nrow.names(outliers_sgd1_table) <- properties_to_latex(outliers_sgd1_table$variable)\n##\n## create xtable object\noutliers_sgd1_xtable <- xtable(outliers_sgd1_table[,-1], digits=0)\n##\n## and print as booktabs\nbooktabs.xtable(outliers_sgd1_xtable) \n##\n## ########################################################\n@\n\\end{table}\n\n\n\n\\subsection{Sampling}\n\\subsubsection{Resampling methods}\nPreviously each sampled horizon was considered separately.  \n Alternative subsampling paradigms  include sample (i) profiles or (ii) projects to provide more realistic subsets for training and validation.\n Another approach is to order the variable from lowest to highest and sample every third row as a validation data set.\n\n<<functions-sampling-methods, echo = F, cache = T, results = hide, depends = functions-basic>>=\n## ########################################################\n## functions for sampling by a second identifying column ##\n## such as profiles or projects                          ##\n## also functions for sampling by ordering the variable  ##\n## ########################################################\nsample_by <- function(variable, value, .rate, .by, .id_column, ...){\n   ## get sample ids\n  other_args <- list(...)\n  id_samples <- other_args[[.id_column]]\n  ##\n  ## get the identifier by which we are sampling\n  id_by <- as.character(other_args[[.by]])\n  ##\n  ## how many unique identifiers\n  unique_by <- unique(id_by)\n  ##\n  ## how many samples?\n  n_samples <- floor(.rate* length(unique_by))\n  ## which samples\n  sampled_by_ids <- sample( unique_by, size = n_samples)\n  ##\n  ## which are in the training set\n  rows_training <- which(id_by %in% sampled_by_ids)\n  ##\n  ## which are in the validation set\n  rows_validation <- seq(along = id_by)[-rows_training]\n  ##\n  ## by_ids\n  id_validation <- id_by[rows_validation]\n  id_training <- id_by[rows_training]\n  ##\n  ## the data sets\n  data_training <- value[rows_training]\n  data_validation <- value[rows_validation]\n  data_all <- value\n  ##\n  ## make a named list\n  training_info <- llist(data_all,data_training, data_validation, id_validation, id_training, id_samples, rows_validation,rows_training)\n  ## \n  ## rename and save\n  .property <- unique(as.character(variable))\n  training_info$variable <- .property\n  obj_name <- paste(.property,'by',.by,'training_info',sep='_')\n  save_as(.object = training_info, .file_path = .save_path, .name = obj_name)\n  ##\n  ## return the named list\n  return(training_info)\n  ##\n  }\n## ########################################################\nsample_ordered <- function(variable, value, .every = 3, .id_column, ...){\n   ## get sample ids\n  other_args <- list(...)\n  id_samples <- other_args[[.id_column]]\n  ## \n  ## get the order of the values\n  order_value <- order(value)\n  ##\n  ## order the sample ids by the values\n  ordered_samples <- id_samples[order_value]\n  ##\n  ## create full vector\n  all_samples <- seq(along = ordered_samples)\n  ##\n  ## subset every third for a validation data set\n  rows_training <- all_samples[(all_samples %% .every) > 0]\n  rows_validation <-  all_samples[(all_samples %% .every) == 0]\n  ##\n  ## create the data sets\n  data_all <- sort(value)\n  data_training <- data_all[(all_samples %% .every) > 0]\n  data_validation <- data_all[(all_samples %% .every) == 0]\n  ##\n  ## create list\n  training_info <- llist(data_all,data_training, data_validation, rows_validation, rows_training)\n  training_info$id_samples <- ordered_samples\n  ##\n  ## rename and save\n  .property <- unique(as.character(variable))\n  training_info$variable <- .property\n  obj_name <- paste(.property,'ordered','training_info',sep='_')\n  save_as(.object = training_info, .file_path = .save_path, .name = obj_name)\n  ##\n  ## return the named list\n  return(training_info)\n  ##\n  }\n## ########################################################\n@\n\n\n<<analysis-sampling-by, echo = F, cache = T, results = hide; dependson = functions-sampling-methods>>=\n## ########################################################\n## sampling by profiles, projects                        ##\n## visual outliers removed                               ##\n## ########################################################\n## \n## load required data\nmolten_soil_data_no_v_o <- load_as(.file_path = saved_data_directory, .name = 'molten_soil_no_v_o')\n##\n## sampling by profiles\nsampled_by_profiles <- dlply(molten_soil_data_no_v_o, .(variable), splat(sample_by), .rate = 0.7,.by = 'user_pedon_id', .id_column = 'row_index' )\n##\n## sampling by projects\nsampled_by_projects <- dlply(molten_soil_data_no_v_o, .(variable), splat(sample_by), .rate = 0.7,.by = 'lab_project_name', .id_column = 'row_index' )\n##\n## save objects\nsave_as(.object = sampled_by_projects, .file_path = training_info_directory, .name = 'sampled_by_projects')\n##\nsave_as(.object = sampled_by_profiles, .file_path = training_info_directory, .name = 'sampled_by_profiles')\n##\n## clean up\nrm(list = c('sampled_by_projects','sampled_by_profiles'))\n##\ninvisible(gc())\n##\n## ########################################################\n@\n\n\n<<analysis-sampling-mahalanobis-outliers-and-ordered, cache = T, echo = F, results = hide, dependson = functions-sampling-methods;analysis-mahalanobis-outlier-detection >>=\n## ########################################################\n## sampling by ordering non-multivariate outliers        ##\n## ########################################################\n##\n## load the required data\nreflectance_sgd1_outliers <- load_as( .file_path = training_info_directory, .name = 'reflectance_sgd1_outliers')\n##\nabsorbance_sgd1_outliers <- load_as( .file_path = training_info_directory, .name = 'absorbance_sgd1_outliers')\n##\nmolten_soil_data_no_v_o <- load_as(.file_path = saved_data_directory, .name = 'molten_soil_no_v_o')\n##\n## join and remove the potential outliers\nsoil_data_no_reflect_sgd1_outliers <- subset(join(molten_soil_data_no_v_o,reflectance_sgd1_outliers), potential_outlier == F)\n##\nsoil_data_no_abs_sgd1_outliers <- subset(join(molten_soil_data_no_v_o,absorbance_sgd1_outliers), potential_outlier == F)\n##\n## ordered sampling for reflectance and absorbance\nordered_sample_no_reflect_sgd1_outliers <- dlply(soil_data_no_reflect_sgd1_outliers, .(variable), splat(sample_ordered), .every = 3, .id_column = 'row_index' )\n##\nordered_sample_no_abs_sgd1_outliers <- dlply(soil_data_no_abs_sgd1_outliers, .(variable), splat(sample_ordered), .every = 3, .id_column = 'row_index' )\n##\n## save objects\nsave_as(.object = ordered_sample_no_reflect_sgd1_outliers, .file_path = training_info_directory, .name = 'ordered_sample_no_reflect_sgd1_outliers')\nsave_as(.object = ordered_sample_no_abs_sgd1_outliers, .file_path = training_info_directory, .name = 'ordered_sample_no_abs_sgd1_outliers')\n##\n## clean up\nrm(list = c('ordered_sample_no_abs_sgd1_outliers', 'ordered_sample_no_reflect_sgd1_outliers',  'soil_data_no_reflect_sgd1_outliers', 'soil_data_no_abs_sgd1_outliers', 'molten_soil_data_no_v_o', 'reflectance_sgd1_outliers', 'absorbance_sgd1_outliers'  ))\ninvisible(gc())\n##\n## ########################################################\n@\n\n<<analysis-models-cubist-sampling-methods, echo = F, cache = T, results = hide; dependson = analysis-sampling-mahalanobis-outliers-and-ordered; analysis-sampling-by  >>=\n## ########################################################\n## re fit cubist models with different sampling methods  ##\n## by Profile                                            ##\n## by profiles                                           ##\n## ordered sampling with outliers removed                ##  \n## ########################################################\n##\n## load required data\nordered_sample_reflect <- load_as(.file_path = training_info_directory, .name = 'ordered_sample_no_reflect_sgd1_outliers')\n##\nordered_sample_abs <- load_as(.file_path = training_info_directory, .name = 'ordered_sample_no_abs_sgd1_outliers')\n##\nsampled_by_projects <- load_as(.file_path = training_info_directory, .name = 'sampled_by_projects')\n##\nsampled_by_profiles <- load_as(.file_path = training_info_directory, .name = 'sampled_by_profiles')\n##\n## create names for coefficients\ncoef_names <-paste('w', seq(400, 2450, by = 10), sep = '') \n##\n## create argument lists for fitting cubist models to \n## SG filtered-D1 reflectance and absorbance\ncubist_by_projects_arguments <- c(llply(sampled_by_projects, function(x,y){c(x,as.list(y))},  y = filter_sg_sub10_files[2,]), llply(sampled_by_projects, function(x,y){c(x,as.list(y))},   y = filter_sg_sub10_files[4,]))\n##\ncubist_by_profiles_arguments <- c(llply(sampled_by_profiles, function(x,y){c(x,as.list(y))},   y = filter_sg_sub10_files[2,]), llply(sampled_by_profiles, function(x,y){c(x,as.list(y))},  y = filter_sg_sub10_files[4,]))\n##\ncubist_ordered_arguments <- c(llply(ordered_sample_reflect, function(x,y){c(x,as.list(y))},  y = filter_sg_sub10_files[2,]), llply(ordered_sample_abs, function(x,y){c(x,as.list(y))},  y = filter_sg_sub10_files[4,]))\n##\n## fit the models and get summaries \nby_projects_cubist_summaries <-  ldply(cubist_by_projects_arguments,.fun = splat(fit_cubist), .coef_names = coef_names, .keep = 1:206, .save_path = cubist_models_directory, .committees = 5, .sampling = 'by_projects')\n##\nby_profiles_cubist_summaries <-  ldply(cubist_by_profiles_arguments,.fun = splat(fit_cubist), .coef_names = coef_names, .keep = 1:206, .save_path = cubist_models_directory, .committees = 5, .sampling = 'by_profiles')\n##\nordered_cubist_summaries <-  ldply(cubist_ordered_arguments,.fun = splat(fit_cubist), .coef_names = coef_names, .keep = 1:206, .save_path = cubist_models_directory, .committees = 5, .sampling = 'mvo_ordered')\n##\n## save\nsave_as(.object = by_profiles_cubist_summaries, .file_path = cubist_models_directory, .name = 'by_profiles_cubist_summaries')\n##\nsave_as(.object = by_projects_cubist_summaries, .file_path = cubist_models_directory, .name = 'by_projects_cubist_summaries')\n##\nsave_as(.object = ordered_cubist_summaries, .file_path = cubist_models_directory, .name = 'ordered_cubist_summaries')\n##\n## clean up\nrm(list = c('ordered_cubist_summaries', 'by_profiles_cubist_summaries', 'by_projects_cubist_summaries', 'cubist_by_profiles_arguments', 'cubist_by_projects_arguments', 'cubist_ordered_arguments', 'ordered_sample_reflect', 'ordered_sample_abs', 'sampled_by_projects',' sampled_by_profiles') )\n##\ninvisible(gc())\n##\n## ########################################################\n@\n\n\n\\begin{table}[tbp]\n\\caption{RMSE for validation and training data sets using cubist in conjunction with profile sampling.}\n\\label{table:cubistRMSEProfiles}\n\\centering\n<<table-model-summaries-cubist-by-profile, echo = F, results =tex, cache = F; dependson = functions-basic;analysis-models-cubist-sampling-methods >>=\n## ########################################################\n## create a table summarizing the cubist fit             ##\n## by profiles sampling                                   ##\n## ########################################################\n##\n## load data\ncubist_summaries <- load_as(.file_path = cubist_models_directory, .name = 'by_profiles_cubist_summaries')\n##\n## reformulate results table\ncubist_table <- cast(reshape2:::melt.data.frame(cubist_summaries, measure.var = 'RMSE', variable.name = 'statistic', id.var = c('.type', 'dataset', 'variable')), variable  ~.type +dataset)\n##\n## set row names for xtable / latex\nrow.names(cubist_table) <- properties_to_latex(cubist_table$variable)\n##\n## create nicer looking column names\nnames(cubist_table)[-1] <-laply(str_split(names(cubist_table)[2:5], pattern = '_'), function(x){paste(x[1], ' (', x[2],')',sep='')})\n##\n## latexify table using xtable\ncubist_xtable <- xtable(cubist_table[,-1], digits=3)\n##\n## and print this  using booktabs format\nbooktabs.xtable(cubist_xtable)\n##\n## clean up\nrm(list = c('cubist_summaries', 'cubist_table','cubist_xtable'))\ninvisible(gc())\n##\n## ########################################################\n@\n\\end{table}\n\n\\begin{table}[tbp]\n\\caption{RMSE for validation and training data sets using cubist in conjunction with project sampling.}\n\\label{table:cubistRMSEProjects}\n\\centering\n<<table-project-summary, echo = F, results =tex, cache = F, dependson = functions-basic;analysis-models-cubist-sampling-methods >>=\n## ########################################################\n## create a table summarizing the cubist fit             ##\n## by project sampling                                   ##\n## ########################################################\n##\n## load data\ncubist_summaries <- load_as(.file_path = cubist_models_directory, .name = 'by_projects_cubist_summaries')\n##\n## reformulate results table\ncubist_table <- cast(reshape2:::melt.data.frame(cubist_summaries, measure.var = 'RMSE', variable.name = 'statistic', id.var = c('.type', 'dataset', 'variable')), variable  ~.type +dataset)\n##\n## set row names for xtable / latex\nrow.names(cubist_table) <- properties_to_latex(cubist_table$variable)\n##\n## create nicer looking column names\nnames(cubist_table)[-1] <-laply(str_split(names(cubist_table)[2:5], pattern = '_'), function(x){paste(x[1], ' (', x[2],')',sep='')})\n##\n## latexify table using xtable\ncubist_xtable <- xtable(cubist_table[,-1], digits=3)\n##\n## and print this  using booktabs format\nbooktabs.xtable(cubist_xtable)\n##\n## clean up\nrm(list = c('cubist_summaries', 'cubist_table','cubist_xtable'))\ninvisible(gc())\n##\n## ########################################################\n@\n\\end{table}\n\n\\begin{table}[tbp]\n\\caption{RMSE for validation and training data sets using cubist in conjunction with ordered sampling with multivariate outliers removed.}\n\\label{table:cubistRMSEMVO}\n\\centering\n<<table-MVO-summary, echo = F, results =tex, cache = F,dependson = functions-basic;analysis-models-cubist-sampling-methods >>=\n## ########################################################\n## create a table summarizing the cubist fit             ##\n## ordered sampling                                   ##\n## ########################################################\n##\n## load data\ncubist_summaries <- load_as(.file_path = cubist_models_directory, .name = 'ordered_cubist_summaries')\n##\n## reformulate results table\ncubist_table <- cast(reshape2:::melt.data.frame(cubist_summaries, measure.var = 'RMSE', variable.name = 'statistic', id.var = c('.type', 'dataset', 'variable')), variable  ~.type +dataset)\n##\n## set row names for xtable / latex\nrow.names(cubist_table) <- properties_to_latex(cubist_table$variable)\n##\n## create nicer looking column names\nnames(cubist_table)[-1] <-laply(str_split(names(cubist_table)[2:5], pattern = '_'), function(x){paste(x[1], ' (', x[2],')',sep='')})\n##\n## latexify table using xtable\ncubist_xtable <- xtable(cubist_table[,-1], digits=3)\n##\n## and print this  using booktabs format\nbooktabs.xtable(cubist_xtable)\n##\n## clean up\nrm(list = c('cubist_summaries', 'cubist_table','cubist_xtable'))\ninvisible(gc())\n##\n## ########################################################\n@\n\\end{table}\n\n\n\n\n\n\\subsection{Wavelets}\nUsing the deviations from the continuum, the spectra are now periodic (starting and ending at 0). We choose a waveband window $[404,2451]$ as it dyadic ($2048 = 2^{11}$)\n\n\nWe present a number of approaches, one using the continuum-removed spectra one on the raw Absorbance values and another on the derivatives.\n The wavelet coefficients are used in a generalized validation procedure where, having removed the multivariate outliers, we order the samples and removed every third value as a validation data set.\n\\subsubsection{Continuum-removed spectra}\nHere, we calculate the wavelet coefficients for a Daubechies wavelet function with 2 vanishing moments. We use the deviations from the continuum of the reflectance. \n<<functions-filter-wavelets, echo = F, cache = T, results = hide, depends = functions-basic>>=\n## ########################################################\nget_wavelet_coefficients <- function(wd_object,.lev,max  = 2048 ,offset = 404, ... ){\n  ##\n  coefs <- accessD(wd_object,.lev)\n  ##\n  names(coefs) <- name_bandcentre(.lev, max = max, offset = offset)\n  ##\n  coefs\n  ##\n}\n## ########################################################\nget_bandcentre <- function(.lev,max  = 2048 ,offset = 404){\n  ## identify the bandcentres for a level of wavelet\n  ## decomposition\n  ##\n  offset + seq( max / 2^(.lev + 1), max, by = max / 2^(.lev) )\n  ##\n}\n## ########################################################\nname_bandcentre <-  function(.lev,max  = 2048 ,offset = 404){\n  ## Get a name that identifies the scale and bandcentre\n  ## for a wavelet coefficients\n  ## \n  ## get the bandcentre\n  .level <- get_bandcentre(.lev, max = max, offset = offset)\n  ##\n  ## create the name\n  paste('w', .level, .lev, sep = '.')\n  ##\n}\n## ########################################################\nfilter_wavelets <- function(.spectra_path, .spectra_name, .filter, rows_training, rows_validation, id_samples, value, variable,.keep, .scales, .save_path, ...){\n  ## a function for filtering wavelets using wd\n  ##\n  ## load spectra\n  spectra <- load_as(.file_path = .spectra_path, .name  = .spectra_name)[,.keep]\n  ##\n  ## calculate the wavelet decomposition -- returns\n  ## a list of wavelet decompositions\n  ## one for each row (spectrum)\n  wavelet_interval <- alply(spectra, 1,wd, bc = 'interval', filter.number = 2, family = 'DaubExPhase', min.scale = 2 )\n  ## \n  ## extract the wavelet coefficients\n  wavelet_coefficients <- ldply(wavelet_interval, function(x){unlist(llply(.scales, get_wavelet_coefficients , wd_object = x))})\n  ##\n  ## name the objects and save  \n  wavelet_name <- paste(.spectra_name, 'wavelet', sep = '_')\n  ##\n  coefficients_name <- paste(.spectra_name, 'wavelet_coefficients', sep = '_')\n  ##\n  save_as(wavelet_interval, .file_path = .save_path, .name = wavelet_name)\n  ##\n  save_as(wavelet_coefficients, .file_path = .save_path, .name = coefficients_name)\n  ##\n  ## return the coefficients\n  return(wavelet_coefficients)\n  ##\n}\n## ########################################################\n@\n\n<<analysis-filter-wavelets, echo = T, results = hide, cache = T, dependson = functions-filter-wavelets>>=\n## ##################################################### ##\n## Perform wavelet-on-the-interval decomposition         ##\n## Debaucheries wavelets with 2 vanishing moments        ##\n## ##################################################### ##\n\n  ## calculate the variance (removing the subset rows)\n  coefVars <- apply(wvCoefs[.subRows,], 2, var)\n  ## and the order\n  varOrder <- order(coefVars, decreasing = T)\n  ## save\n  saveObjectAs(processedSpectra, .fname = .resultsNames$processedSpectra$.fname, \n               .name = .resultsNames$processedSpectra$.name)\n  saveObjectAs(waveletInterval, .fname = .resultsNames$waveletInterval$.fname, \n               .name = .resultsNames$waveletInterval$.name)\n  saveObjectAs(wvCoefs, .fname = .resultsNames$wvCoefs$.fname, \n               .name = .resultsNames$wvCoefs$.name)\n  saveObjectAs(varOrder, .fname = .resultsNames$varOrder$.fname, \n               .name = .resultsNames$varOrder$.name)\n  return(.resultsNames)\n  }\n\ncrWaveletSetUp <- list(.spectra = list(.fname = 'savedData/reflectAllSG.RData',.name = 'reflectAllSG'),\n                  .preProcess = list(.type = 'CR', .range = 404:2451),\n                  .subRows = whichDataOut,\n                  .resultsNames = list(processedSpectra = list(.fname = 'savedData/reflectSGDyadicCR.RData', \n                                                               .name = 'reflectSGDyadicCR'),\n                                      waveletInterval = list(.fname = 'savedData/reflectSGCRWaveletList.RData',\n                                                             .name = 'reflectSGCRWaveletList'),\n                                      wvCoefs = list(.fname = 'savedData/reflectSGCRWaveletCoefs.RData', \n                                                     .name = 'reflectSGCRWaveletCoefs'),\n                                      varOrder =  list(.fname = 'savedData/reflectSGCRWaveletVarOrder.RData', \n                                                       .name = 'reflectSGCRWaveletVarOrder')))\n## fit the model\ncrWaveletResultFiles <- splat(waveletFunction)(crWaveletSetUp)      \n@\n\n<<wavelets-cr-Models, echo = F, cache = T, results = hide,eval=F>>=\n.modelSummary <- function(.which,observed,predicted){\n     modelResid <- (observed-predicted)[.which]\n     data.frame(MAE = mean(abs(modelResid)), RMSE = sqrt(mean(modelResid^2)))\n  }\n\n\nwaveletModelQPFit <- function(.number, wvCoefs,.variable, .inData, trainingInfo){\n  ## create training data\n  trainingData <- data.frame(.var = .inData[trainingInfo$trainingRows,.variable], wvCoefs[trainingInfo$trainingRows,])\n  ## create formula\n  waveFormula <- makeFormula(.names = names(wvCoefs)[1:.number], .y = '.var')\n  ## create model\n  waveModel <- lm(waveFormula,trainingData )\n  ## calculate adjusted R squared\n  adjR2 <- summary(waveModel)$adj.r.squared\n  ## make prediction\n  wavePredicted <- predict(waveModel,newdata = wvCoefs)\n  \n  modelSummary <- ldply(trainingInfo,.fun= '.modelSummary', observed = .inData[,.variable], predicted = wavePredicted)\n  modelSummary$adjR2 <- adjR2\n  modelSummary$variable <- as.character(.variable)\n  modelSummary$number <- .number\n  modelSummary\n}\n\n\nwaveletModelQP <- function(.processedSpectra,.variable,.maxNumber,.subRows,.inData, .varOrder,.wvCoefs){\n  ## load spectra\n  spectra <- loadRename(.processedSpectra$.fname, .processedSpectra$.name)\n  ## make pls \n  plsData <- data.frame(.y = .inData[,.variable],spectra = I(as.matrix(spectra)))\n  .plsFormula <- as.formula(paste('.y~',paste(names(plsData)[-1],collapse='+')))\n  ## get outliers\n  outlierInfo <- outlierMV(.Data = plsData, .sc = 5, .subRows = .subRows, .dataCol=1, .formula = .plsFormula)\n  rm(plsData)\n  ## get training info\n  trainingInfo <- orderedSample(.y = .inData[,.variable],.mvOut = outlierInfo)\n  ##trainingInfo\n  ## load varOrder\n  varOrder <- loadRename(.varOrder$.fname, .varOrder$.name)\n  ## load coefficients\n  wvCoefs <- loadRename(.wvCoefs$.fname, .wvCoefs$.name)[,varOrder[1:.maxNumber]]\n  ## fit the model\n  results <- ldply(1:.maxNumber, function(.number){res <- waveletModelQPFit(.number=.number, wvCoefs = wvCoefs, .variable =.variable, .inData = .inData, trainingInfo = trainingInfo); res})\n  results\n  }\n\nwaveletResultsCR <- ldply(whichProperties, function(.variable){results <- waveletModelQP(.processedSpectra = crWaveletResultFiles$processedSpectra, .variable= .variable,.inData = soilData, .subRows = whichDataOut,.maxNumber = 200 ,.varOrder =crWaveletResultFiles$varOrder,.wvCoefs = crWaveletResultFiles$wvCoefs); results})\n\n## and save\nsave(waveletResultsCR, file = 'savedData/models/waveletResultsCR.RData')\nrm(waveletResultsCR)\n@\n\nThe root mean-square-errors of prediction are shown in figure~\\ref{fig:crWavelets} for models constructed with up to the first 200 coefficients as ordered by variance.\n\\begin{figure}[tbp]\n\\centering\n<<cr-waveletResults, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6, fig.height = 9,background = 1;1;1, cache = T>>=\ngetRMSE <- function(.x){\n  data.frame(nCoefs = which.min(.x$RMSE),rmse = min(.x$RMSE),  variable = unique(.x$variable))\n  }\nwaveletModelPlot <- function(resultsFname, resultsName,nCoefList){\n  ## load results \n  .results <- loadRename( .fname = resultsFname, resultsName)\n  ## make latex friendly\n  .results$latexVar <- sapply(.results$variable,propertiesLatex)\n  ## calculate the best model by RMSE (validation)\n  nCoefDF <- ddply(.results,.(latexVar,.id), getRMSE)\n  ## save for later use\n  saveObjectAs(nCoefDF, .fname = nCoefList$.fname, .name = nCoefList$.name)\n  \n  #make the plot (validation rows only)  \n  ggplot(subset(.results,.id=='validationRows')) + \n    facet_wrap(~latexVar,scales = 'free',ncol=2) +\n    geom_point(aes(x=number,y = RMSE),size=1) + \n    geom_segment(data = subset(nCoefDF,.id=='validationRows'), aes(x = nCoefs, xend = nCoefs, y = rmse, yend = 0)) + \n    geom_segment(data = subset(nCoefDF,.id=='validationRows'), aes(x = nCoefs, xend = 0, y = rmse, yend = rmse)) + \n    xlab('Number of DWT coefficients')+\n    ylab('RMSE') + \n    scale_x_continuous(expand = c(0,0))+\n    opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(),strip.background = theme_blank()) \n}\n\nwaveletModelPlot(resultsFname = 'savedData/models/waveletResultsCR.RData', resultsName = 'waveletResultsCR',nCoefList = list(.fname = 'savedData/nCoefsCRDF.RData',.name = 'nCoefsCRDF') )\n\n\n\n@\n\\caption{Root mean-square-error of predictions for various soil properties. The models were fitted by  multiple linear regression with quadratic polynomials of the wavelet coefficients ordered by their variance and performed sequentially by adding one coefficient at a time. Wavelet coefficients were calculated on the deviations from the reflectance continuum.  }\\label{fig:crWavelets}\n\\end{figure}\n\n\\subsubsection{Absorbance}\nThe root mean-square-errors of prediction are shown in figure~\\ref{fig:absWavelets} for models constructed with up to the first 200  DWT coefficients as ordered by variance.\n\n\n<<wavelets-Abs, echo = F, cache = T, results = hide>>=\n## set up the required files etc\nabsWaveletSetUp <- list(.spectra = list(.fname = 'savedData/absAll.RData',.name = 'absAll'),\n      .preProcess = list(.type = 'none', .range = (350:2500 %in% 404:2451) ), .subRows = whichDataOut,\n      .resultsNames = list(processedSpectra = list(.fname = 'savedData/absDyadic.RData',\n        .name = 'absDyadic'), waveletInterval = list(.fname = 'savedData/absWaveletList.RData', \n        .name = 'absWaveletList'), wvCoefs = list(.fname = 'savedData/absWaveletCoefs.RData', \n        .name = 'absWaveletCoefs'), varOrder =  list(.fname = 'savedData/absWaveletVarOrder.RData', \n        .name = 'absWaveletVarOrder')))\n\n## run the wavelet fitting\nabsWaveletResultFiles <- splat(waveletFunction)(absWaveletSetUp)\n\n\n@\n\n<<wavelets-Abs-Models, echo = F, cache = T, results = hide>>=\n## create quadratic polynomial models\n## source('racaFunctions.R')\nwaveletResultsAbs <- ldply(whichProperties, function(.variable){results <- waveletModelQP(.processedSpectra = absWaveletResultFiles$processedSpectra, .variable= .variable,.inData = soilData, .subRows = whichDataOut,.maxNumber = 200 ,.varOrder =absWaveletResultFiles$varOrder,.wvCoefs = absWaveletResultFiles$wvCoefs); results})\n\n## and save\nsave(waveletResultsAbs, file = 'savedData/models/waveletResultsAbs.RData')\n## clean up\nrm(waveletResultsAbs)\n@\n\n\n\\begin{figure}[tbp]\n\\centering\n<<abs-waveletResults, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6, fig.height = 9,background = 1;1;1, cache = T>>=\ninvisible(gc())\nwaveletModelPlot('savedData/models/waveletResultsAbs.RData','waveletResultsAbs',nCoefList = list(.fname = 'savedData/nCoefsAbsDF.RData',.name = 'nCoefsAbsDF') )\n@\n\\caption{Root mean-square-error of predictions for various soil properties. The models were fitted by  multiple linear regression with quadratic polynomials of the wavelet coefficients ordered by their variance and performed sequentially by adding one coefficient at a time. Wavelet coefficients were calculated on the absorbance.}\\label{fig:absWavelets}\n\\end{figure}\n\n\\subsubsection{First derivative}\n\n<<wavelets-SGD1, echo = F, cache = T, results = hide>>=\n## set utp file names and input for wavelets\nabsSGD1WaveletSetUp <- list(.spectra = list(.fname = 'savedData/absAllSGD1.RData',.name = 'absAllSGD1'),\n      .preProcess = list(.type = 'SGD1', .range = (350:2500 %in% 404:2451) ), .subRows = whichDataOut,\n      .resultsNames = list(processedSpectra = list(.fname = 'savedData/absSGD1Dyadic.RData',\n        .name = 'absSGD1Dyadic'), waveletInterval = list(.fname = 'savedData/absSGD1WaveletList.RData', \n        .name = 'absSGD1WaveletList'), wvCoefs = list(.fname = 'savedData/absSGD1WaveletCoefs.RData', \n        .name = 'absSGD1WaveletCoefs'), varOrder =  list(.fname = 'savedData/absSGD1WaveletVarOrder.RData', \n        .name = 'absSGD1WaveletVarOrder')))\n## run the wavelet decomposition\nabsSGD1WaveletResultFiles <- splat(waveletFunction)(absSGD1WaveletSetUp)\ninvisible(gc())\n@\n\n<<wavelets-SGD1-Models, echo = F, cache = T, results = hide>>=\n## create quadratic polynomial models\nwaveletResultsAbsSGD1 <- ldply(whichProperties, function(.variable){\n  results <- waveletModelQP(.processedSpectra = absSGD1WaveletResultFiles$processedSpectra, \n              .variable = .variable,.inData = soilData, .subRows = whichDataOut,.maxNumber = 200 , \n              .varOrder =absSGD1WaveletResultFiles$varOrder,.wvCoefs = absSGD1WaveletResultFiles$wvCoefs)\n  results})\n## and save\nsave(waveletResultsAbsSGD1, file = 'savedData/models/waveletResultsAbsSGD1.RData')\nrm(waveletResultsAbsSGD1)\ninvisible(gc())\n@\n\nThe root mean-square-errors of prediction are shown in figure~\\ref{fig:SGD1Wavelets} for models constructed with up to the first 200 coefficients as ordered by variance.\n\\begin{figure}[tbp]\n\\centering\n<<SGD1-waveletResults, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6, fig.height = 9,background = 1;1;1, cache = T>>=\n## load data\nwaveletModelPlot('savedData/models/waveletResultsAbsSGD1.RData','waveletResultsAbsSGD1',nCoefList = list(.fname = 'savedData/nCoefsAbsSGD1DF.RData',.name = 'nCoefsAbsSGD1DF') )\n@\n\\caption{Root mean-square-error of predictions for various soil properties. The models were fitted by  multiple linear regression with quadratic polynomials of the wavelet coefficients ordered by their variance and performed sequentially by adding one coefficient at a time. Wavelet coefficients were calculated on the  first derivative of the absorbance  }\\label{fig:SGD1Wavelets}\n\\end{figure}\n\n\\subsubsection{Wavelet coefficients used}\nFor each of the spectrum types we calculated the proportion of the selected wavelet coefficients by scale. These results are summarized in figure~\\ref{fig:coefSelect}. The number of coefficients corresponding to the lowest RMSE is shown in table~\\ref{table:wvResults}.\n\n\n\\begin{table}[tbp]\n\\caption{Optimal models based on wavelet coefficients}\n\\label{table:wvResults}\n\\centering\n<<table-wavelet-results, echo=F, results = tex, cache = F, dependson = setup-libraries,warning=F,message=F>>=\n## format a table using booktabs in LaTeX\nnCoefLists <- list(list(.fname = 'savedData/nCoefsCRDF.RData',.name = 'nCoefsCRDF', .type = 'CR'),\n                   list(.fname = 'savedData/nCoefsAbsDF.RData',.name = 'nCoefsAbsDF', .type = 'Abs'), \n                   list(.fname = 'savedData/nCoefsAbsSGD1DF.RData',.name = 'nCoefsAbsSGD1DF', .type= 'SGD1'))\n\n\nwvTable <- do.call('cbind',llply(nCoefLists, function(.list){\n  .results <- splat(loadRename)(.list[c('.fname','.name')])\n  .latexVar <- .results[.results$.id == 'validationRows','latexVar']\n  .results <- .results[ .results$.id == 'validationRows',c('nCoefs','rmse')]\n  .results <- data.frame(.results)\n  row.names(.results) <- .latexVar#\n  .results\n  }))\n#   cbind(nCoefAbsDF[,2:3],nCoefCRDF[,2:3],nCoefSGD1DF[,2:3])\nnames(wvTable) <- paste(rep(c('Coef','RMSE'),2),rep(c('(CR)','(Abs)','(D1)'),each=2))\n# rownames(wvTable) <- nCoefAbsDF$latexVar\nwvXTable <- xtable(wvTable, digits=3)\n  print(wvXTable,sanitize.text.function = function(x){x},floating=FALSE,hline.after=NULL,\n                  add.to.row=list(pos=list(-1,0, nrow(xTab)),command=c('\\\\toprule ','\\\\midrule ','\\\\bottomrule ')))\n@\n\\end{table}\n\n\n\n\n\n<<wavelets-coefficients-used, echo = F, cache = T, results = hide>>=\n\nlibrary(stringr)\n## CR wavelets\n\nselectedScales <- function(.in,...){\n  ## get frequencies for each scale\n  perLevel <- tabDF(table(.in$level),ind = 'level')\n  ## find proportion for each scale\n  perLevel$proportion <- with(perLevel, count / 2^as.numeric(as.character(level)))\n  ## return\n  perLevel\n}\n\ngetCoefficients <- function(wvCoefs.fname,wvCoefs.name,varOrder.fname,varOrder.name, nCoefs, variable,...){\n   wvCoefs <- loadRename(as.character(wvCoefs.fname),as.character(wvCoefs.name))\n  ## var order\n  varOrder <-  loadRename(as.character(varOrder.fname),as.character(varOrder.name))\n  ## identify number\n  ## get Names (trimmed to approriate level based on .number)\n  coefNames <- names(wvCoefs)[varOrder[1:nCoefs]]\n  ## get band centre and level\n centreLevel <- ldply(str_split( coefNames,pattern='[[:punct:]]'), \n                       function(x){data.frame(bandCentre=x[2],level = x[3])})\n   }\n\n\n\n## get coefficients\n## add in the appropriate levels\nnCoefLists[[1]]$resultsList <- crWaveletResultFiles\nnCoefLists[[2]]$resultsList <- absWaveletResultFiles\nnCoefLists[[3]]$resultsList <- absSGD1WaveletResultFiles\n\nnCoefLists[[1]]$resultsListAll <- list(.fname = 'savedData/models/waveletResultsCR.RData', \n                                       .name = 'waveletResultsCR')\nnCoefLists[[2]]$resultsListAll <- list(.fname = 'savedData/models/waveletResultsAbs.RData', \n                                       .name = 'waveletResultsAbs')\nnCoefLists[[3]]$resultsListAll <- list(.fname = 'savedData/models/waveletResultsAbsSGD1.RData', \n                                       .name = 'waveletResultsAbsSGD1')\n\nnCoefs <- ldply(nCoefLists, function(.list){ \n .results <- with(.list, loadRename(.fname, .name))\n .allResults <- with(.list$resultsListAll,loadRename(.fname, .name) )\n  .return <- data.frame(.results[ .results$.id == 'validationRows',c('nCoefs','variable')],\n                        .type = .list$.type, wvCoefs.fname = .list$resultsList$wvCoefs$.fname, \n                        wvCoefs.name = .list$resultsList$wvCoefs$.name,  \n                        varOrder.fname = .list$resultsList$varOrder$.fname, \n                        varOrder.name = .list$resultsList$varOrder$.name )\n .return$RMSEP <- .results[ .results$.id == 'validationRows','rmse']\n .return$RMSET <-  daply(.return, .(variable), function(.x,.all){\n   .number = .x$nCoefs\n   .variable = .x$variable\n   RSMET <- .all[(.all$variable == .variable)&(.all$.id == 'trainingRows')&(.all$number== .number),'RMSE']\n    },.all = .allResults)\n \n .return$latexVar <- sapply(as.character(.return$variable),propertiesLatex)\n \n for(.name in c('variable','.type', paste(rep(c('wvCoefs','varOrder'), each = 2), rep(c('.fname','.name'),2),sep=''))){  \n  .return[,.name] <- as.character(.return[,.name])}\n .return\n   })\n\nwaveletQPSelected <- adply(nCoefs,1, splat(getCoefficients))[,c('.type','variable','latexVar','bandCentre','level')]\nresultsWV <- ddply(waveletQPSelected, .(.type,variable), selectedScales)#adply(nCoefs,1, splat(selectedCoefficients))\n\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-coeffsused, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6, fig.height = 9,background = 1;1;1, cache = T,depends = wavelets-coefficients-used>>=\n## the plot\nresultsWV$olevel <-ordered(as.numeric(as.character(resultsWV$level)))\nggplot(resultsWV) + \n  facet_wrap(.type~.property,ncol=5) + \n  geom_bar(aes(x=olevel,y=proportion*100))+\n  xlab('DWT scale')+\n  ylab('vis-NIR coefficients retained / \\\\%') + \n  scale_y_continuous(expand = c(0,0))+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(),strip.background = theme_blank())\n\n@\n\\caption{Proportion of selected wavelet coefficients by scale for models of various soil properties. CR -- devations from the convex hull, Abs -- absorbance, SGD1 -- first derivative of smoothed absorbance.}\\label{fig:coefSelect}\n\\end{figure}\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-coeffsusedID, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6, fig.height = 9,background = 1;1;1, cache = T,depends = wavelets-coefficients-used>>=\n## the plot\ntheme_set(theme_bw())\nggplot(waveletQPSelected) +\n  facet_grid(latexVar~.type)+\n  geom_rect(aes(xmin=factor2Numeric(bandCentre) - 2048 / 2^(factor2Numeric(level)+1),\n     ymin = factor2Numeric(level)-0.5, ymax = factor2Numeric(level) + 0.5,\n                 xmax =factor2Numeric(bandCentre) + 2048 / 2^(factor2Numeric(level)+1) ),\n             colour='black', fill = 'grey90',size=0.05) + \n  scale_x_continuous('Wavelength / nm', expand = c(0.01,0)) + \n  scale_y_continuous('Scale', expand = c(0.01,0)) + \n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank())\n  \n@\n\\caption{Tile plot showing selected wavelet coefficients by scale for models of various soil properties. CR -- devations from the convex hull, Abs -- absorbance, SGD1 -- first derivative of smoothed absorbance.}\\label{fig:coefSelectID}\n\\end{figure}\n\n\n\\subsection{Selecting wavelet coefficients using cubist}\nAs a final approach we consider selecting the wavelet coefficients using Cubist. To reduce the computational burden, we consider the first 250 coefficients as ordered by their variance. \n\n\n<<som-functions,echo=F,cache=T,results=hide>>=\n.modelSummary <- function(.which,observed,predicted){\n     modelResid <- (observed-predicted)[.which]\n     data.frame(MAE = mean(abs(modelResid)), RMSE = sqrt(mean(modelResid^2)))\n  }\n\nwaveletModelCubistFit <- function(.number, wvCoefs,.variable, .inData, trainingInfo){\n  ## create training data\n  trainingData <-  .inData[trainingInfo$trainingRows,.variable]\n  ## create formula\n  waveFormula <- makeFormula(.names = names(wvCoefs)[1:.number], .y = '.var')\n  ## create model\n  waveModel <- lm(waveFormula,trainingData )\n  ## calculate adjusted R squared\n  adjR2 <- summary(waveModel)$adj.r.squared\n  ## make prediction\n  wavePredicted <- predict(waveModel,newdata = wvCoefs)\n  \n  modelSummary <- ldply(trainingInfo,.fun= .modelSummary, observed = .inData[,.variable], predicted = wavePredicted)\n  modelSummary$adjR2 <- adjR2\n  modelSummary$variable <- as.character(.variable)\n  modelSummary$number <- .number\n  modelSummary\n}\n\n\nwvCubistFit <- function(.processedSpectra,.variables,.maxNumber, .subRows, .varOrder, .wvCoefs){\n  results <- ldply(.variables,waveletModelCubist, .coefs=.coefs,.varOrder=.varOrder,.FT = .fit,.inData=.inData,.processedSpectra=.processedSpectra)\n  results\n  }\n@\n\n<<wavelet-cubist-fitting, cache = T, results= hide,echo=F>>=\n.modelSummary <- function(.which,observed,predicted){\n     modelResid <- (observed-predicted)[.which]\n     data.frame(MAE = mean(abs(modelResid)), RMSE = sqrt(mean(modelResid^2)))\n  }\n\nwaveletModelCubist <- function(.processedSpectra,.variable,.maxNumber,.subRows,.inData, .varOrder,.wvCoefs,.model,.cubistInfo){\n  ## load spectra\n  spectra <- loadRename(.processedSpectra$.fname, .processedSpectra$.name)\n  ## make pls \n  plsData <- data.frame(.y = .inData[,.variable],spectra = I(as.matrix(spectra)))\n  .plsFormula <- as.formula(paste('.y~',paste(names(plsData)[-1],collapse='+')))\n  ## get outliers\n  outlierInfo <- outlierMV(.Data = plsData, .sc = 5, .subRows = .subRows, .dataCol=1, .formula = .plsFormula)\n  rm(plsData)\n  ## get training info\n  trainingInfo <- orderedSample(.y = .inData[,.variable],.mvOut = outlierInfo)\n  ##trainingInfo\n  ## load varOrder\n  varOrder <- loadRename(.varOrder$.fname, .varOrder$.name)\n  ## load coefficients\n  wvCoefs <- loadRename(.wvCoefs$.fname, .wvCoefs$.name)[,varOrder[1:.maxNumber]]\n  ## training y\n  yTraining <- .inData[trainingInfo$trainingRows,.variable]\n  ##  X\n  xAll <- data.frame(wvCoefs, wvCoefs^2)\n  ## named\n  names(xAll) <- paste(rep(c('l','sq'),each=.maxNumber),names(wvCoefs)[1:.maxNumber],sep='.')\n  ## subset training data\n  xTraining <- xAll[trainingInfo$trainingRows,]\n  ## fit model\n  cubistModel <- cubist(y = yTraining, x = xTraining, committees = .cubistInfo$committees,\n                        control = .cubistInfo$control)\n ## save model\n  saveObjectAs(cubistModel, .fname = file.path(.model$.fpath,paste(.variable,.model$.name,'.RData',sep='')), .name = paste(.variable,.model$.name,sep=''))\n  ## validation and model checking\n  ## predict\n  cubistPredicted <- predict(cubistModel, newdata = xAll)\n  ## make summary\n  modelSummary <- ldply(trainingInfo,.fun= '.modelSummary', observed = .inData[,.variable], predicted = cubistPredicted)\n  ## save usage\n  variableUsage <- cubistModel$usage\n  ## return  \n  return(llist(trainingInfo, modelSummary, variableUsage)) \n  }\n\n\n\nwaveletCubistCR <- llply(whichProperties, function(.variable){\n  results <- waveletModelCubist(.processedSpectra = crWaveletResultFiles$processedSpectra, \n                .variable= .variable,.inData = soilData, .subRows = whichDataOut,.maxNumber = 250 ,\n                .varOrder =crWaveletResultFiles$varOrder,.wvCoefs = crWaveletResultFiles$wvCoefs,\n                .model = list(.fpath = 'savedData/models',.name ='WaveletCubistModelCR' ),\n                .cubistInfo = list(committees = 5, control = cubistControl()))\n  results})\n\nwaveletCubistAbs <- llply(whichProperties, function(.variable){\n  results <- waveletModelCubist(.processedSpectra = absWaveletResultFiles$processedSpectra, \n                .variable= .variable,.inData = soilData, .subRows = whichDataOut,.maxNumber = 250 ,\n                .varOrder =absWaveletResultFiles$varOrder,.wvCoefs = absWaveletResultFiles$wvCoefs,\n                .model = list(.fpath = 'savedData/models',.name ='WaveletCubistModelAbs' ),\n                .cubistInfo = list(committees = 5, control = cubistControl()))\n  results})\n\nwaveletCubistabsSGD1 <- llply(whichProperties, function(.variable){\n  results <- waveletModelCubist(.processedSpectra = absSGD1WaveletResultFiles$processedSpectra, \n                .variable= .variable,.inData = soilData, .subRows = whichDataOut,.maxNumber = 250 ,\n                .varOrder =absSGD1WaveletResultFiles$varOrder,.wvCoefs = absSGD1WaveletResultFiles$wvCoefs,\n                .model = list(.fpath = 'savedData/models',.name ='WaveletCubistModelabsSGD1' ),\n                .cubistInfo = list(committees = 5, control = cubistControl()))\n  results})\n\ninvisible(gc())\n@\n\n<<cubist-wavelet-summary, echo = F, results = hide, cache = T>>=\n## name the lists correctly\nlibrary(stringr)\nnames(waveletCubistCR) <- names(waveletCubistAbs) <- names(waveletCubistabsSGD1) <- whichProperties\ninvisible(gc())\n## a function to extract the summaries from the lists of results\ngetSummaries <- function(.list,using,type = 'WAV-CUB'){\n        summary <- data.frame(.list$modelSummary, using = using, type= type)\n         names(summary)[1] <- 'dataset'\n         summary\n        }\n## extract the model summaries\nwaveletCubistResults <- rbind(ldply(waveletCubistCR,getSummaries, using = 'C'), \n                              ldply(waveletCubistAbs,getSummaries, using = 'A'),\n                              ldply(waveletCubistabsSGD1,getSummaries, using = 'A-D1'))\n## latexify variable names\nwaveletCubistResults$latexVar <- sapply(as.character(waveletCubistResults$.id), propertiesLatex)\n## a function to make factors numeric (level '100' becomes  100)\nfactor2Numeric <- function(.x){as.numeric(as.character(.x))}\n\n## a function to get the usage and extract the coeffient names and levels\ngetUsage <- function(variableUsage, using,.maxNumber = 250,...){\n  ## get the useage\n  summary <- data.frame(variableUsage,using = using )\n  ## subset those variables used in either conditions or models\n  results <-  subset(summary, ((Conditions > 0) | (Model >0)))\n  ## convert variable names to wavelet coefficients and levels\n  coefsUsed <- adply(results,1, splat(function(Coniditions,Model,Variable,...){\n    x <- unlist(str_split(Variable,pattern='[[:punct:]]'))\n   data.frame(power = x[1], band =factor2Numeric(x[3]), \n              level = factor2Numeric(x[4]))}))\n  coefsUsed\n\n}\n  ## get the usage for each variable and pre-processing\nwaveletCubistUsage <- rbind(ldply(waveletCubistCR,splat(getUsage), using = 'C'), \n                              ldply(waveletCubistAbs,splat(getUsage), using = 'A'),\n                              ldply(waveletCubistabsSGD1,splat(getUsage), using = 'A-D1'))\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-coeffsused-cubist, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6, fig.height = 9,background = 1;1;1, cache = T,depends = cubist-wavelet-summary>>=\n## find the proportions for each level\nidentifyUniqueWavebands <- function(power,level,...){\n  data.frame(power = paste(sort(power),collapse=':'), level = unique(level))\n}\n\nproportionByScale <- function(.data) {\n      .unique <- ddply(.data,.(band), splat(identifyUniqueWavebands))\n      perLevel <- tabDF(table(.unique$level),ind='level')\n      perLevel$proportion <- with(perLevel, count / 2^as.numeric(as.character(level)))\n      perLevel}\n\nwaveCubistCoefficientProportions <- ddply(waveletCubistUsage,.(.id,using), proportionsByScale)\n## latexift\nwaveCubistCoefficientProportions$latexVar <- sapply(waveCubistCoefficientProportions$.id, propertiesLatex)\n## make olevel and ordered factor\nwaveCubistCoefficientProportions$olevel <-ordered(factor2Numeric(waveCubistCoefficientProportions$level))\n\nggplot(waveCubistCoefficientProportions) + \n  facet_wrap(using~latexVar,ncol=5) + \n  geom_bar(aes(x=olevel,y=proportion*100))+\n  xlab('DWT scale')+\n  ylab('vis-NIR coefficients retained / \\\\%') + \n  scale_y_continuous(expand = c(0,0))+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(),strip.background = theme_blank())\n\n@\n\\caption{Proportion of selected wavelet coefficients  by scale for models of various soil properties developed using Cubist. CR -- devations from the convex hull, Abs -- absorbance, SGD1 -- first derivative of smoothed absorbance.}\\label{fig:coefSelectCubist}\n\\end{figure}\n\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-coeffsID-cubist, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 6, fig.height = 9,background = 1;1;1, cache = T,depends = cubist-wavelet-summary>>=\n## find the proportions for each level\nwaveCubistUsagePlot <- ddply(waveletCubistUsage, .(.id,using), function(.data){\n  cast(melt(.data,measure.var = c('band'),id.var = c('power','Conditions','Model','level')), value + Conditions + Model + level~power )\n})\n\nwaveCubistUsagePlot$latexVar <- sapply(waveCubistUsagePlot$.id, propertiesLatex)\nggplot(waveCubistUsagePlot) +\n  facet_grid(latexVar~using)+\n  geom_rect(aes(xmin=factor2Numeric(l) - 2048 / 2^(factor2Numeric(level)+1),\n     ymin = factor2Numeric(level)-0.5, ymax = factor2Numeric(level) + 0.5,\n                 xmax =factor2Numeric(l) + 2048 / 2^(factor2Numeric(level)+1), fill =replace(Conditions,Conditions==0,NA) ),\n             colour= 'black',size=0.05,) + \n  scale_x_continuous('Wavelength / nm', expand = c(0.01,0)) + \n  scale_y_continuous('Scale', expand = c(0.01,0)) + \n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), strip.background = theme_blank())\n\n@\n\\caption{Tile plot showing selected wavelet coefficients  by scale for models of various soil properties developed using Cubist. CR -- devations from the convex hull, Abs -- absorbance, SGD1 -- first derivative of smoothed absorbance.}\\label{fig:coefSelectIDCubist}\n\\end{figure}\n\n\\section{Some more on mineralogy}\n\\subsection{Iron oxides}\nSection~\\ref{section:Fe} set out a procedure to calculate the abundance of the iron oxide minerals Hematite and Goethite. \nFigure~\\ref{fig:IronOxides} shows how these abundance calculations (based on a single waveband peak) compare with the laboratory measured iron oxide.\n\n<<fe-mineralogy-comparisons, cache = T, results = hide, echo = F>>=\n## load mineral abundances\nfor(.mineral in c('Hematite','Goethite')){\n  load(paste('savedData/abundance',.mineral,'.RData',sep=''))\n}\n\nfeBaseplot <- ggplot(data.frame(feCD = soilData$feCD[whichDataOut], goeth = abundanceGoethite[whichDataOut], hem = abundanceHematite[whichDataOut]))\n\ngoethitePlot <- feBaseplot + geom_point(aes(x = feCD , y = goeth),size=0.75) + \n  xlab('Measured Fe') +\n  ylab('Goethite Abundance')+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank()) +\n  scale_x_continuous(expand = c(0,0), limit = c(0.5,16)) + \n  scale_y_continuous(expand = c(0,0))\n\nhematitePlot <- feBaseplot + geom_point(aes(x = feCD , y = hem),size=0.75) + \n  xlab('Measured Fe') +\n  ylab('Hematite Abundance')+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank()) +\n  scale_x_continuous(expand = c(0,0), limit = c(0.5,16)) + \n  scale_y_continuous(expand = c(0,0))\n\nfeBothPlot <- feBaseplot + geom_point(aes(x = feCD , y = goeth +hem),size=0.75) + \n  xlab('Measured Fe') +\n  ylab('Total Abundance')+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank()) +\n  scale_x_continuous(expand = c(0,0), limit = c(0.5,16)) + \n  scale_y_continuous(expand = c(0,0))\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-feSpectroscopy, fig=T, dev=tikz,echo=F, out.width = \\textwidth, fig.width = 7, fig.height = 8,background = 1;1;1, cache = T,warning=F>>=\n## the plot\ngrid.arrange(goethitePlot, hematitePlot, feBothPlot, nrow =3)\n@\n\\caption{Laboratory-measured Iron oxide compared with spectroscopic measurements of abundance}\\label{fig:IronOxides}\n\\end{figure}\n\n\n\\subsection{Clay minerals}\n<<clay-mineralogy-comparisons, cache = T, results = hide, echo = F>>=\n## load mineral abundances\nfor(.mineral in names(clayList)){\n  load(paste('savedData/abundanceRelative',.mineral,'.RData',sep=''))\n}\n\nclayBaseplot <- ggplot(data.frame(clay = soilData$clay[whichDataOut], \n                                 Smectite = abundanceRelativeSmectite[whichDataOut], \n                                  Illite = abundanceRelativeIllite[whichDataOut],\n                                  Kaolinite = abundanceRelativeKaolinite[whichDataOut],\n                                  CEC= soilData$cecNH4[whichDataOut]))\n\nsmectiteCECPlot <- clayBaseplot + geom_point(aes(x = CEC , y = clay * Smectite, colour = clay))  +\nscale_x_continuous(expand =c(0,0), xlim= c(0,100)) +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_colour_gradientn('Clay / %', colours = brewer.pal(n=5,name = 'RdGy'))+\n  xlab('CEC') +\n  ylab('Smectite x Clay')+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), legend.title = theme_text( size = 12 * 0.8, hjust = 0, face = 'plain'))\n\nkaoliniteCECPlot <-clayBaseplot + geom_point(aes(x = CEC , y = clay * Kaolinite, colour = clay))  +\n  scale_x_continuous(expand =c(0,0), xlim= c(0,100)) +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_colour_gradientn('Clay / %', colours = brewer.pal(n=5,name = 'RdGy'))+\n  xlab('CEC') +\n  ylab('Kaolinite x Clay')+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), legend.title = theme_text( size = 12 * 0.8, hjust = 0, face = 'plain'))\n\nilliteCECPlot <- clayBaseplot +geom_point(aes(x = CEC , y = clay * Illite,colour=clay))  +\nscale_x_continuous(expand =c(0,0), xlim= c(0,100)) +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_colour_gradientn('Clay / %', colours = brewer.pal(n=5,name = 'RdGy'))+\n  xlab('CEC') +\n  ylab('Illite x Clay')+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), legend.title = theme_text( size = 12 * 0.8, hjust = 0, face = 'plain')) \n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-claySpectroscopy, fig=T, dev=png,echo=F, out.width = \\textwidth, fig.width = 7, fig.height = 8,background = 1;1;1, cache = T,warning=F,dpi=300>>=\n## the plot\n\n## create the legend for the combined plot\nmineralCECLegend <- ggplotGrob(kaoliniteCECPlot + opts(keep ='legend_box')) \n## one needs to provide the legend with a well-defined width\nlegendCEC <- gTree(children = gList(mineralCECLegend), cl = 'legendGrob')\nwidthDetails.legendGrob <- function(x) unit(3, \"cm\")\n## arrange the various plots, legend and axis titles\ngrid.arrange(smectiteCECPlot+ opts(legend.position = 'none') + xlab(''), \n             kaoliniteCECPlot + opts(legend.position = 'none')+ xlab(''),\n            illiteCECPlot  +  opts(legend.position = 'none')+ xlab(''),\n            legend = legendCEC,\n             nrow = 3,\n            sub = textGrob('CEC', vjust = -0.5))\n## add the a, b , c labels\n\n\n@\n\\caption{Laboratory-measured CEC compared with spectroscopic measurements of clay mineral abundance abundance}\\label{fig:clayCEC}\n\\end{figure}\n\n\\section{Identifying the best model for each property}\nWe select compare the best models for each property with respect to the RMSE on the training and validation data sets. \nThe choices are Cubist (based on outlier-removed ordered sampling, CUB-MVO), quadratic polynomials on wavelet coefficients (WAV-QP). Where appropriate we consider absorbance (ABS), reflectance (REF), continuum-removed (CR) and first derivatives (SGD1).\n\n\n<<identify-models, echo = F, results = hide, cache = T, eval = T>>=\n## cubist only\nmvoResults <- subset(samplingResults, sampling =='MVO', select = c(MAE,MSE,dataset,using,variable))\n## convert the labels to common \nlevels(mvoResults$using)  <- c('RD1','AD1')\nmvoResults$using <- as.character(mvoResults$using)\n## add type\nmvoResults$type = 'CUB'\n## create RMSE\nmvoResults$RMSE <- sqrt(mvoResults$MSE)\n## add latexVar\nmvoResults$latexVar <- sapply(as.character(mvoResults$variable), propertiesLatex)\n\n## wavelet QP\nresultsWavelet <- melt(nCoefs, id.vars = c('.type','variable','latexVar'),measure.vars = c('RMSEP','RMSET'))\nnames(resultsWavelet)[4:5] <- c('dataset','RMSE')\nresultsWavelet$variable <- as.character(resultsWavelet$variable)\n## convert labels to common\nresultsWavelet$using  <- as.character(factor(resultsWavelet$.type, levels = unique(resultsWavelet$.type),labels = c('C','A','AD1')))\n\nlevels(resultsWavelet$dataset) <- c('validation','training')\nresultsWavelet$dataset <- as.factor(as.character(resultsWavelet$dataset))\n## add type\nresultsWavelet$type = 'WAV-QP'\n\n## wavelet + Cubist\n## convert variable to character\nwaveletCubistResults$variable <- as.character(waveletCubistResults$.id)\n## convert labels to common\nlevels(waveletCubistResults$using)  <- c('C','A','AD1')\nwaveletCubistResults$dataset <- factor(waveletCubistResults$dataset,labels = c('training','validation'))\n\n\n\nwhichCols <- intersect(intersect(names(resultsWavelet),names(mvoResults)), names(waveletCubistResults))\n\n\nbestModels <- rbind(resultsWavelet[,whichCols],mvoResults[,whichCols],waveletCubistResults[,whichCols])\n\n@\n\n\nThe root mean-square-error for the training and validation data sets are shown in figure~\\ref{fig:bestModel}. \nThe models identified with lowest RMSE are shown in table~\\ref{table:bestModel}. In later sections, we use the best model, identified using the validation data set for prediction. The cubist models using the wavelet coefficients from the raw absorbance tend to provide the best models in terms of the validation RMSE whilst the continuum removed reflectance provides the majority of the best models identified using the training RMSE.\nThe vast majority of {\\em best} models are developed using Cubist in conjunction with the wavelet coefficients. \n\n\\begin{figure}[tbp]\n\\centering\n<<best-model-figure, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 7, fig.height = 9,background = 1;1;1, cache = T>>=\nggplot(bestModels, aes(y = RMSE, x = using, shape = dataset,color=type)) +\n  geom_point(size=2)+\n  facet_wrap(~latexVar,scales='free',ncol=2) +\n  scale_x_discrete('Spectral pre-processing') +\n  opts(strip.background = theme_blank(), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank())\n@\n\\caption{Root mean-square-error of predictions for various soil properties and methods }\\label{fig:bestModel}\n\\end{figure}\n\n\\begin{table}[tbp]\n\\caption{Best model for each soil property. Refer to figure~\\ref{fig:bestModel} for the data. The number in brackets is the RMSE.}\n\\label{table:bestModel}\n\\centering\n<<identify-best, echo=F, cache = F, results=tex>>=\nbestModelTable <- ddply(bestModels, .(variable,dataset), function(.data){whichMin <- which.min(.data$RMSE); min = min(.data$RMSE); .data[whichMin,c('type','using','RMSE')]})\n\nbestTable <- cast(melt(adply(bestModelTable,1,function(.row){paste(.row$type,'-',.row$using, ' (', round(.row$RMSE,3), ')', sep='')}), measure.var = 'V1', id.var = c('variable','dataset')), variable~dataset)\nrownames(bestTable) <- sapply(as.character(bestTable$variable),propertiesLatex)\n\nprint(xtable(data.frame(bestTable[,2:3])),sanitize.text.function = function(x){x},floating=FALSE,hline.after=NULL,\n                  add.to.row=list(pos=list(-1,0, nrow(xTab)),command=c('\\\\toprule ','\\\\midrule ','\\\\bottomrule ')))\n\n@\n\\end{table}\n\n<<save-best-model,echo=F,results=hide,cache=T>>=\n## save the best models\nbestModelID <- cast(melt(bestModelTable, measure.var = c('type'), id.var = c('variable', 'dataset')), \n                    variable ~ dataset)[,c(1,3)]\nnames(bestModelID) <- c('variable','type')\n\nbestModelID$using <- cast(melt(bestModelTable, measure.var = c('using'), id.var = c('variable', 'dataset')), \n                    variable ~ dataset)$validation\n\n list(.fpath = 'savedData/models',.name ='WaveletCubistModelabsSGD1' )\n\nbestModelSwitchType <- function(.x){\n  switch(as.character(.x), 'WAV-CUB' = 'WaveletCubistModel', 'CUB' = 'MVOCubist')\n}\n\nbestModelSwitchUsing <- function(.using){\n   switch(as.character(.using),'C' = 'CR', 'AD1' = 'absSGD1','A' = 'Abs')\n}\n\nfileEnd <- function(.x){\n  switch(as.character(.x),'CUB' = '' ,'WAV-CUB' = '.RData' ) \n}\n\ngetBestModel <- function(variable,type,using){\n  .spectra <- bestModelSwitchType(as.character(type))\n  .id <-  bestModelSwitchUsing(as.character(using))\n   .end <- fileEnd(type)\n  .fname <- paste('savedData/models/',variable,.spectra,.id,.end,sep='')\n  .name <-  paste(variable,.spectra,.id,sep='')\n  if((type == 'CUB')&(using) == 'AD1'){.id = 'abs'\n                .fname <- paste('savedData/models/',variable,.id,.spectra,.end,sep='')\n  .name <-  paste(variable,.id,.spectra,sep='')                           }\n  \n  bestModel <- loadRename(.fname,.name)\n   bestModel.fname <- paste('savedData/models/',variable,'BestModel','.RData',sep='')\n   bestModel.name <- paste(variable,'bestModel',sep='')\n  saveObjectAs(bestModel,bestModel.fname ,bestModel.name)\n  \n  data.frame(variable, type, using, .fname,.name, bestModel.fname, bestModel.name, .spectra,.id)\n }\n  \n\n\nbestModelsFiles <- adply(bestModelID,1,splat(getBestModel))\n@\n\n\n<<make-predictions, cache = T, results = hide, echo= F>>=\nswitchWaveCoefs <- function(.using){\n  switch(as.character(.using), 'C' = 'reflectSGCR', 'A' = 'abs','AD1' = 'absSGD1')\n}\n## make predictions using the best models\nbestPredictions <- alply(bestModelsFiles,1, function(.row, .which, .keep, .maxNumber){\n  ## load best model\n  bestModel <- loadRename(as.character(.row$bestModel.fname),as.character(.row$bestModel.name))\n  ## get the appropriate new data\n  if(as.character(.row$type) == 'CUB'){\n  ## file name\n    spec.fname <- paste('savedData/', as.character(.row$.id), '10SGD1.RData', sep = '' )\n    spec.name <- paste( as.character(.row$.id), '10SGD1', sep = '')\n  ## load\n    newSpectra <- loadRename(spec.fname, spec.name)\n  ## subset  \n    newSpectra <- as.data.frame(newSpectra[.which,.keep])\n    ## name columns\n    names(newSpectra) <- paste('w',seq(400,2450,by=10),sep='')\n  }\n  if(as.character(.row$type) == 'WAV-CUB'){\n   ## load order\n    .pre <- switchWaveCoefs(.row$using)\n    varOrder <- paste(.pre,'WaveletVarOrder',sep='')\n    coefs <- paste(.pre, 'WaveletCoefs',sep='')\n   .varOrder <- loadRename(.fname = paste('savedData/', varOrder, '.RData',sep = ''), .name = varOrder)\n  ## and variance   \n    .coefs <- loadRename(.fname = paste('savedData/', coefs, '.RData',sep = ''),\n                         .name = coefs)[,.varOrder[1:.maxNumber]]\n  ## make dataframe\n   newSpectra <- as.data.frame(cbind(.coefs[.which,],.coefs[.which,]^2))\n  ## names\n  names(newSpectra) <- paste(rep(c('l','sq'),each=.maxNumber),1:.maxNumber,sep='.')\n  }\n  ## predict\n  prediction <- predict(bestModel, newdata = newSpectra)\n  ## return\n  prediction},.which= 1:3171, .keep = keep10, .maxNumber = 250 )\n## name the list\nnames(bestPredictions) <- as.character(bestModelsFiles$variable)\n## convert to a data frame\nbestDF <- as.data.frame(do.call('cbind',bestPredictions))\n## add the ID column\nbestDF$userPedonId <- soilData$userPedonId\n## save\nsaveObjectAs(bestDF, .fname = 'savedData/predictedProperties.RData', 'predictedProperties')\n@\n\n\n\n\\subsection{Distributions of predicted and measured properties}\nIt is important to consider whether the distribution of the predicted and measured properties are similar. These distributions are shown in figure~\\ref{fig:predictedProperties}.\n\\begin{figure}[tbp]\n\\centering\n<<predicted-distributions, fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 7, fig.height = 9,background = 1;1;1, cache = T,warning = F>>=\n## make a big data frame\nbigDF <- rbind(data.frame(melt(soilData,id.var = 'userPedonId',measure.var = whichProperties),type='measured'),\n      data.frame(melt(bestDF[whichDataOut,],id.var = 'userPedonId',measure.var = whichProperties ), type= 'predicted'))\n## convert to latexed variable names\nbigDF$latexVar <- sapply(as.character(bigDF$variable), propertiesLatex)\n## make the plot\nggplot(bigDF, aes(x= value,colour = type)) + \n  geom_density(size=1) + \n  facet_wrap(~latexVar,scale='free',ncol=2) +\n # scale_x_continuous(expand = c(0,0))+\n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(),strip.background = theme_blank())\nsave(bigDF, file = 'savedData/bigDF.RData')               \nrm(bigDF)\ninvisible(gc())\n@\n\\caption{Distributions of the measured and predicted soil properties}\\label{fig:predictedProperties}\n\\end{figure}\n\n\n\\section{Classification of spectra using predicted properties}\nIn previous sections we have used Cubist to build models for various soil properties based on vis-NIR spectra.  \nIn this section we include the predicted soil properties with a cubist model to develop a model for total Carbon. \nIt is hoped that the predicted properties may form the basis of a classification.\nIn addition to the predicted properties, we include the various mineral abunances calculated from the spectra as well as horizon information.\n The horizon information is {\\em included} as the \\raca{} project will record this information.\n \n \\subsection{Cubist model including predicted properties}\n\nHere we include the predicted properties, clay and iron oxide abundances, soil colour {\\sc RGB} and the top depth of the horizon. \nWe also include the various forms of spectra -- Absorbance (1st Derivative) sampled at every 10th wave band, and wavelet coefficients for  deviations from the reflectance continuum,  Absorbance, and Absorbance (1st derivative).\n\nFor each spectral preprocessing we identify the multivariate outliers, and perform ordered sampling to create a validation data set.\n\n<<classified-model,cache = T, echo=F, results = hide>>=\n## SDG1\n## load best predictions\n.modelSummary <- function(.which,observed,predicted){\n     modelResid <- (observed-predicted)[.which]\n     data.frame(MAE = mean(abs(modelResid)), RMSE = sqrt(mean(modelResid^2)))\n  }\n\ncubistClassify <- function(.spectra,.pre, .properties, .y,.subRows,.spectraUse, .cubistInfo){\n  ## load spectra\n  spectra <- loadRename(.spectra$.fname, .spectra$.name)\n  if(.pre$wave == F){\n    spectra <- spectra[,.spectra$.keep]\n  }\n  ## identify outliers\n  plsData <- data.frame(.y,.properties,spectra = I(as.matrix(spectra)))\n  .plsFormula <- as.formula(paste('.y~',paste(names(plsData)[-1],collapse='+')))\n  outlierInfo <- outlierMV(.Data = plsData, .sc = 5, .subRows = .subRows, .dataCol=1, .formula = .plsFormula)\n  rm(plsData)\n  ## get training info\n  trainingInfo <- orderedSample(.y,.mvOut = outlierInfo)\n  rm(spectra)\n  ## load wavelets if required\n  if(.pre$wave == T){\n    .keep <- loadRename(.spectraUse$varOrder$.fname, .spectraUse$varOrder$.name)[1:.pre$.maxNumber]\n    inspectra <-loadRename(.spectraUse$wvCoefs$.fname, .spectraUse$wvCoefs$.name)[,.keep]\n    namesCol <- names(inspectra)\n    spectra  <- as.data.frame(cbind(inspectra,inspectra^2))\n    names(spectra) <- paste(rep(c('l','sq'), each = length(namesCol)),namesCol,sep='.')\n  }\n ## load spectra If required\n  if(.pre$wave == F){\n    spectra <- as.data.frame(loadRename(.spectraUse$.fname, .spectraUse$.name)[,.spectraUse$.keep])\n }\n  ## the training and validation data sets\n  yTraining <- .y[trainingInfo$trainingRows]\n  yValidation <- .y[trainingInfo$validationRows]\n  # all X\n  xAll <- data.frame(.properties, spectra)\n  # training and validation X\n  xTraining <- xAll[trainingInfo$trainingRows,]\n  xValidation <- xAll[trainingInfo$validationRows,]\n  ## fit model\n  cubistModel <- cubist(y = yTraining, x = xTraining, committees = .cubistInfo$committees,\n                        control = .cubistInfo$control)\n ## save model\n  saveObjectAs(cubistModel, .fname = .pre$.fname, .name = .pre$.name)\n  ## validation and model checking\n  ## predict\n  cubistPredicted <- predict(cubistModel, newdata = xAll[.subRows,])\n  cP <- rep(NA,nrow(xAll))\n  cP[.subRows] <- cubistPredicted\n  ## make summary\n  modelSummary <- ldply(trainingInfo,.fun= '.modelSummary', observed = .y, predicted = cP)\n  ## save usage\n  variableUsage <- cubistModel$usage\n  ## return  \n  return(llist(trainingInfo, modelSummary, variableUsage))\n }\n\n## whichCols\nusingProperties <- whichProperties[-c(which(whichProperties %in% c('totalC','userPedonId','estOrgC')))]\n## load rgb\nsoilColour <- loadRename('savedData/rgbCols.RData','rgbCols')\n## load clay mineralogy\nfor(.mineral in c('Illite','Smectite','Kaolinite')){\n  .mineralAbundance <- loadRename(paste('savedData/abundanceRelative',.mineral,'.RData',sep=''),\n                                  paste('abundanceRelative',.mineral,sep=''))\n  assign(paste('abundance',.mineral,sep=''),.mineralAbundance)\n}\nfor(.mineral in c('Hematite','Goethite')){\n  .mineralAbundance <- loadRename(paste('savedData/abundance',.mineral,'.RData',sep=''),\n                                  paste('abundance',.mineral,sep=''))\n  assign(paste('abundance',.mineral,sep=''),.mineralAbundance)\n}\n\n## to do with horizonTop\n.which <- (1:3171)[!is.na(soilData$horizonTop)]\noutliers <- c(1981,3144)\nwhichOut <- .which[-c(which(.which %in% outliers))]\n\n\n## all properties\nallProperties <- data.frame(horizonTop = soilData$horizonTop, bestDF[,usingProperties],\n                    soilColour[,c('red','green','blue')], abundanceKaolinite, abundanceSmectite,\n                    abundanceIllite, abundanceHematite, abundanceGoethite)\n\n\nabsSGD1 <- list(.spectra = list(.fname = 'savedData/abs10SGD1.RData', .name = 'abs10SGD1', .keep = keep10), \n                 .pre = list(wave = F,.fname = 'savedData/models/totalCClassAbsD1.RData', .name = 'totalCClassAbsD1'),\n                 .y = soilData$totalC, .subRows = whichOut, \n                  .spectraUse = list(.fname = 'savedData/abs10SGD1.RData', .name = 'abs10SGD1', .keep = keep10), \n                 .cubistInfo = list(committees = 5, control = cubistControl()))\n\n\nabsCRWav <- list(.spectra = crWaveletSetUp$.resultsNames$processedSpectra, \n                 .pre = list(wave = T, .fname = 'savedData/models/totalCClassCRWav.RData',\n                             .name = 'totalCClassCRWav', .maxNumber = 250 ),\n                 .y = soilData$totalC, .subRows = whichOut, \n                  .spectraUse = crWaveletSetUp$.resultsNames[c('wvCoefs','varOrder')],\n                 .cubistInfo = list(committees = 5, control = cubistControl()))\n\n\nabsWav <- list(.spectra = absWaveletSetUp$.resultsNames$processedSpectra, \n                 .pre = list(wave = T, .fname = 'savedData/models/totalCClassCRWav.RData',\n                             .name = 'totalCClassCRWav', .maxNumber = 250 ),\n                 .y = soilData$totalC, .subRows = whichOut, \n                  .spectraUse = absWaveletSetUp$.resultsNames[c('wvCoefs','varOrder')],\n                 .cubistInfo = list(committees = 5, control = cubistControl()))\n\n\nabsSGD1Wav <- list(.spectra = absSGD1WaveletSetUp$.resultsNames$processedSpectra, \n                 .pre = list(wave = T, .fname = 'savedData/models/totalCClassCRWav.RData',\n                             .name = 'totalCClassCRWav', .maxNumber = 250 ),\n                 .y = soilData$totalC, .subRows = whichOut, \n                  .spectraUse = absSGD1WaveletSetUp$.resultsNames[c('wvCoefs','varOrder')],\n                 .cubistInfo = list(committees = 5, control = cubistControl()))\n\n\nclassifiedResults <- llply(llist(absSGD1,absCRWav, absWav,absSGD1Wav), splat(cubistClassify), .properties = allProperties)\n@\n\n\\begin{figure}[tbp]\n\\centering\n<<figure-totalC-classfying,  fig=T, dev=tikz,echo=F, out.width = 0.9\\textwidth, fig.width = 7,fig.height=4,cache =T, background = 1;1;1, warning = F, dependson = setup-libraries>>=\nbestTotalC <- ldply(classifiedResults,getSummaries,using='A',type='CUB')\nbestTotalC$type <- as.factor(c(rep('CUB',2),rep('CUB-WAV',6)))\nbestTotalC$using <- rep(c('AD1','C','A','AD1'),each=2)\ntheme_set(theme_bw())\nggplot(bestTotalC,aes(x=interaction(using,type,drop=T),y=RMSE,colour = dataset)) +\n  geom_point() + \n  ylab('RMSE') + \n  xlab('Pre-processing') + \n  opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank())\ninvisible(gc())\n@\n\n<<best-totalC-variables,echo=F,reuslts=hide,cache=T>>=\ngetUsageAllProperties <- function(.list,using,type){\nsummary <- data.frame(.list$variableUsage,using = using, type )\n  ## subset those variables used in either conditions or models\n  results <-  subset(summary, ((Conditions > 0) | (Model >0)))  \n}\n\nusageTotalC <- ldply(classifiedResults,getUsageAllProperties,using='A',type='CUB')\n\n\nusageTotalC$type <- sapply(usageTotalC$.id, function(.x){switch(.x, 'absSGD1' = 'AD1', 'absCRWav' = 'C','absWav' = 'A','absSGD1Wav' = 'AD1')})\nusageTotalC$using <- sapply(usageTotalC$.id, function(.x){switch(.x, 'absSGD1' = 'CUB', 'absCRWav' = 'CUB-WAV','absWav' = 'CUB-WAV','absSGD1Wav' = 'CUB-WAV')})\n@\n\n\\caption{Performance of cubist models including predicted soil property information as well as spectra}\\label{fig:cubistPerform}\n\\end{figure}\n\n\\begin{itemize}\n\\item Do we include horizon depth information? \n\\item Preliminary results show slight decrease:, the best total C MSE pre stratification 1.682 / 2.483 for training / validation using $- {\\rm log} \\left(R \\right)$. Post stratification this reduces below 1.5 for some strata, increases above 2 for others.\n\\end{itemize}\n\n[To Do]\n\\begin{itemize}\n\\item Refine stratification procedure\n\\item Better usage of mineralogical information\n\\end{itemize}\n\n\n\\bibliography{spectraBib}\n\n\n\\end{document}",
    "created" : 1327544304448.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "hash" : "2919466471",
    "id" : "E94888E7",
    "lastKnownWriteTime" : 1327546540,
    "path" : "~/Documents/soil-spectroscopy/knitrTest.Rnw",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "sweave"
}